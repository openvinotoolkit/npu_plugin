// automatically generated by the FlatBuffers compiler, do not modify


#ifndef FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_
#define FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_

#include "flatbuffers/flatbuffers.h"

#include "memoryManagement_generated.h"
#include "software_generated.h"
#include "structure_generated.h"

namespace MVCNN {

struct PPEFixedFunction;
struct PPEFixedFunctionT;

struct PPETask;
struct PPETaskT;

struct NCEInvariantFields;
struct NCEInvariantFieldsT;

struct NCEVariantFields;
struct NCEVariantFieldsT;

struct NCE2Task;
struct NCE2TaskT;

enum NN2Optimization {
  NN2Optimization_NONE = 0,
  NN2Optimization_SQUASHED_CONVOLUTION = 1,
  NN2Optimization_MIN = NN2Optimization_NONE,
  NN2Optimization_MAX = NN2Optimization_SQUASHED_CONVOLUTION
};

inline const NN2Optimization (&EnumValuesNN2Optimization())[2] {
  static const NN2Optimization values[] = {
    NN2Optimization_NONE,
    NN2Optimization_SQUASHED_CONVOLUTION
  };
  return values;
}

inline const char * const *EnumNamesNN2Optimization() {
  static const char * const names[] = {
    "NONE",
    "SQUASHED_CONVOLUTION",
    nullptr
  };
  return names;
}

inline const char *EnumNameNN2Optimization(NN2Optimization e) {
  if (e < NN2Optimization_NONE || e > NN2Optimization_SQUASHED_CONVOLUTION) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesNN2Optimization()[index];
}

enum DPULayerType {
  DPULayerType_CONV = 0,
  DPULayerType_DWCONV = 1,
  DPULayerType_MAXPOOL = 2,
  DPULayerType_AVEPOOL = 3,
  DPULayerType_FCL = 4,
  DPULayerType_ELTWISE = 5,
  DPULayerType_IDENTITY = 6,
  DPULayerType_CMCONV = 7,
  DPULayerType_MIN = DPULayerType_CONV,
  DPULayerType_MAX = DPULayerType_CMCONV
};

inline const DPULayerType (&EnumValuesDPULayerType())[8] {
  static const DPULayerType values[] = {
    DPULayerType_CONV,
    DPULayerType_DWCONV,
    DPULayerType_MAXPOOL,
    DPULayerType_AVEPOOL,
    DPULayerType_FCL,
    DPULayerType_ELTWISE,
    DPULayerType_IDENTITY,
    DPULayerType_CMCONV
  };
  return values;
}

inline const char * const *EnumNamesDPULayerType() {
  static const char * const names[] = {
    "CONV",
    "DWCONV",
    "MAXPOOL",
    "AVEPOOL",
    "FCL",
    "ELTWISE",
    "IDENTITY",
    "CMCONV",
    nullptr
  };
  return names;
}

inline const char *EnumNameDPULayerType(DPULayerType e) {
  if (e < DPULayerType_CONV || e > DPULayerType_CMCONV) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesDPULayerType()[index];
}

enum PPELayerType {
  PPELayerType_STORE = 0,
  PPELayerType_LOAD = 1,
  PPELayerType_CLEAR = 2,
  PPELayerType_NOOP = 3,
  PPELayerType_HALT = 4,
  PPELayerType_ADD = 5,
  PPELayerType_SUB = 6,
  PPELayerType_MULT = 7,
  PPELayerType_RELU = 8,
  PPELayerType_RELUX = 9,
  PPELayerType_LPRELU = 10,
  PPELayerType_MAXIMUM = 11,
  PPELayerType_MINIMUM = 12,
  PPELayerType_CEIL = 13,
  PPELayerType_FLOOR = 14,
  PPELayerType_AND = 15,
  PPELayerType_OR = 16,
  PPELayerType_XOR = 17,
  PPELayerType_NOT = 18,
  PPELayerType_ABS = 19,
  PPELayerType_NEG = 20,
  PPELayerType_POW = 21,
  PPELayerType_EXP = 22,
  PPELayerType_SIGMOID = 23,
  PPELayerType_TANH = 24,
  PPELayerType_SQRT = 25,
  PPELayerType_RSQRT = 26,
  PPELayerType_FLEXARB = 27,
  PPELayerType_MIN = PPELayerType_STORE,
  PPELayerType_MAX = PPELayerType_FLEXARB
};

inline const PPELayerType (&EnumValuesPPELayerType())[28] {
  static const PPELayerType values[] = {
    PPELayerType_STORE,
    PPELayerType_LOAD,
    PPELayerType_CLEAR,
    PPELayerType_NOOP,
    PPELayerType_HALT,
    PPELayerType_ADD,
    PPELayerType_SUB,
    PPELayerType_MULT,
    PPELayerType_RELU,
    PPELayerType_RELUX,
    PPELayerType_LPRELU,
    PPELayerType_MAXIMUM,
    PPELayerType_MINIMUM,
    PPELayerType_CEIL,
    PPELayerType_FLOOR,
    PPELayerType_AND,
    PPELayerType_OR,
    PPELayerType_XOR,
    PPELayerType_NOT,
    PPELayerType_ABS,
    PPELayerType_NEG,
    PPELayerType_POW,
    PPELayerType_EXP,
    PPELayerType_SIGMOID,
    PPELayerType_TANH,
    PPELayerType_SQRT,
    PPELayerType_RSQRT,
    PPELayerType_FLEXARB
  };
  return values;
}

inline const char * const *EnumNamesPPELayerType() {
  static const char * const names[] = {
    "STORE",
    "LOAD",
    "CLEAR",
    "NOOP",
    "HALT",
    "ADD",
    "SUB",
    "MULT",
    "RELU",
    "RELUX",
    "LPRELU",
    "MAXIMUM",
    "MINIMUM",
    "CEIL",
    "FLOOR",
    "AND",
    "OR",
    "XOR",
    "NOT",
    "ABS",
    "NEG",
    "POW",
    "EXP",
    "SIGMOID",
    "TANH",
    "SQRT",
    "RSQRT",
    "FLEXARB",
    nullptr
  };
  return names;
}

inline const char *EnumNamePPELayerType(PPELayerType e) {
  if (e < PPELayerType_STORE || e > PPELayerType_FLEXARB) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesPPELayerType()[index];
}

enum MPE_Mode {
  MPE_Mode_VECTOR = 0,
  MPE_Mode_MATRIX = 1,
  MPE_Mode_VECTOR_FP16 = 2,
  MPE_Mode_MIN = MPE_Mode_VECTOR,
  MPE_Mode_MAX = MPE_Mode_VECTOR_FP16
};

inline const MPE_Mode (&EnumValuesMPE_Mode())[3] {
  static const MPE_Mode values[] = {
    MPE_Mode_VECTOR,
    MPE_Mode_MATRIX,
    MPE_Mode_VECTOR_FP16
  };
  return values;
}

inline const char * const *EnumNamesMPE_Mode() {
  static const char * const names[] = {
    "VECTOR",
    "MATRIX",
    "VECTOR_FP16",
    nullptr
  };
  return names;
}

inline const char *EnumNameMPE_Mode(MPE_Mode e) {
  if (e < MPE_Mode_VECTOR || e > MPE_Mode_VECTOR_FP16) return "";
  const size_t index = static_cast<size_t>(e);
  return EnumNamesMPE_Mode()[index];
}

struct PPEFixedFunctionT : public flatbuffers::NativeTable {
  typedef PPEFixedFunction TableType;
  std::vector<PPELayerType> Ops;
  int32_t Clamp_Low;
  int32_t Clamp_High;
  int8_t Lrelu_Mult;
  uint8_t Lrelu_Shift;
  PPEFixedFunctionT()
      : Clamp_Low(-2147483648),
        Clamp_High(2147483647),
        Lrelu_Mult(1),
        Lrelu_Shift(0) {
  }
};

struct PPEFixedFunction FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef PPEFixedFunctionT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_OPS = 4,
    VT_CLAMP_LOW = 6,
    VT_CLAMP_HIGH = 8,
    VT_LRELU_MULT = 10,
    VT_LRELU_SHIFT = 12
  };
  /// This object describes basic PPE tasks that use
  /// the fixed functionality of the hardware directly.
  /// Up to 16 Operations can be performed, any more and
  /// External reconfiguration is needed.
  const flatbuffers::Vector<uint8_t> *Ops() const {
    return GetPointer<const flatbuffers::Vector<uint8_t> *>(VT_OPS);
  }
  int32_t Clamp_Low() const {
    return GetField<int32_t>(VT_CLAMP_LOW, -2147483648);
  }
  int32_t Clamp_High() const {
    return GetField<int32_t>(VT_CLAMP_HIGH, 2147483647);
  }
  int8_t Lrelu_Mult() const {
    return GetField<int8_t>(VT_LRELU_MULT, 1);
  }
  uint8_t Lrelu_Shift() const {
    return GetField<uint8_t>(VT_LRELU_SHIFT, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_OPS) &&
           verifier.VerifyVector(Ops()) &&
           VerifyField<int32_t>(verifier, VT_CLAMP_LOW) &&
           VerifyField<int32_t>(verifier, VT_CLAMP_HIGH) &&
           VerifyField<int8_t>(verifier, VT_LRELU_MULT) &&
           VerifyField<uint8_t>(verifier, VT_LRELU_SHIFT) &&
           verifier.EndTable();
  }
  PPEFixedFunctionT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PPEFixedFunctionT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<PPEFixedFunction> Pack(flatbuffers::FlatBufferBuilder &_fbb, const PPEFixedFunctionT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PPEFixedFunctionBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_Ops(flatbuffers::Offset<flatbuffers::Vector<uint8_t>> Ops) {
    fbb_.AddOffset(PPEFixedFunction::VT_OPS, Ops);
  }
  void add_Clamp_Low(int32_t Clamp_Low) {
    fbb_.AddElement<int32_t>(PPEFixedFunction::VT_CLAMP_LOW, Clamp_Low, -2147483648);
  }
  void add_Clamp_High(int32_t Clamp_High) {
    fbb_.AddElement<int32_t>(PPEFixedFunction::VT_CLAMP_HIGH, Clamp_High, 2147483647);
  }
  void add_Lrelu_Mult(int8_t Lrelu_Mult) {
    fbb_.AddElement<int8_t>(PPEFixedFunction::VT_LRELU_MULT, Lrelu_Mult, 1);
  }
  void add_Lrelu_Shift(uint8_t Lrelu_Shift) {
    fbb_.AddElement<uint8_t>(PPEFixedFunction::VT_LRELU_SHIFT, Lrelu_Shift, 0);
  }
  explicit PPEFixedFunctionBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  PPEFixedFunctionBuilder &operator=(const PPEFixedFunctionBuilder &);
  flatbuffers::Offset<PPEFixedFunction> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<PPEFixedFunction>(end);
    return o;
  }
};

inline flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunction(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<flatbuffers::Vector<uint8_t>> Ops = 0,
    int32_t Clamp_Low = -2147483648,
    int32_t Clamp_High = 2147483647,
    int8_t Lrelu_Mult = 1,
    uint8_t Lrelu_Shift = 0) {
  PPEFixedFunctionBuilder builder_(_fbb);
  builder_.add_Clamp_High(Clamp_High);
  builder_.add_Clamp_Low(Clamp_Low);
  builder_.add_Ops(Ops);
  builder_.add_Lrelu_Shift(Lrelu_Shift);
  builder_.add_Lrelu_Mult(Lrelu_Mult);
  return builder_.Finish();
}

inline flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunctionDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    const std::vector<uint8_t> *Ops = nullptr,
    int32_t Clamp_Low = -2147483648,
    int32_t Clamp_High = 2147483647,
    int8_t Lrelu_Mult = 1,
    uint8_t Lrelu_Shift = 0) {
  auto Ops__ = Ops ? _fbb.CreateVector<uint8_t>(*Ops) : 0;
  return MVCNN::CreatePPEFixedFunction(
      _fbb,
      Ops__,
      Clamp_Low,
      Clamp_High,
      Lrelu_Mult,
      Lrelu_Shift);
}

flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunction(flatbuffers::FlatBufferBuilder &_fbb, const PPEFixedFunctionT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct PPETaskT : public flatbuffers::NativeTable {
  typedef PPETask TableType;
  std::unique_ptr<TensorReferenceT> scale_data;
  std::unique_ptr<PPEFixedFunctionT> fixed_function;
  std::vector<uint32_t> instruction_list;
  PPETaskT() {
  }
};

struct PPETask FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef PPETaskT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_SCALE_DATA = 4,
    VT_FIXED_FUNCTION = 6,
    VT_INSTRUCTION_LIST = 8
  };
  const TensorReference *scale_data() const {
    return GetPointer<const TensorReference *>(VT_SCALE_DATA);
  }
  const PPEFixedFunction *fixed_function() const {
    return GetPointer<const PPEFixedFunction *>(VT_FIXED_FUNCTION);
  }
  const flatbuffers::Vector<uint32_t> *instruction_list() const {
    return GetPointer<const flatbuffers::Vector<uint32_t> *>(VT_INSTRUCTION_LIST);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_SCALE_DATA) &&
           verifier.VerifyTable(scale_data()) &&
           VerifyOffset(verifier, VT_FIXED_FUNCTION) &&
           verifier.VerifyTable(fixed_function()) &&
           VerifyOffset(verifier, VT_INSTRUCTION_LIST) &&
           verifier.VerifyVector(instruction_list()) &&
           verifier.EndTable();
  }
  PPETaskT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(PPETaskT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<PPETask> Pack(flatbuffers::FlatBufferBuilder &_fbb, const PPETaskT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct PPETaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_scale_data(flatbuffers::Offset<TensorReference> scale_data) {
    fbb_.AddOffset(PPETask::VT_SCALE_DATA, scale_data);
  }
  void add_fixed_function(flatbuffers::Offset<PPEFixedFunction> fixed_function) {
    fbb_.AddOffset(PPETask::VT_FIXED_FUNCTION, fixed_function);
  }
  void add_instruction_list(flatbuffers::Offset<flatbuffers::Vector<uint32_t>> instruction_list) {
    fbb_.AddOffset(PPETask::VT_INSTRUCTION_LIST, instruction_list);
  }
  explicit PPETaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  PPETaskBuilder &operator=(const PPETaskBuilder &);
  flatbuffers::Offset<PPETask> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<PPETask>(end);
    return o;
  }
};

inline flatbuffers::Offset<PPETask> CreatePPETask(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> scale_data = 0,
    flatbuffers::Offset<PPEFixedFunction> fixed_function = 0,
    flatbuffers::Offset<flatbuffers::Vector<uint32_t>> instruction_list = 0) {
  PPETaskBuilder builder_(_fbb);
  builder_.add_instruction_list(instruction_list);
  builder_.add_fixed_function(fixed_function);
  builder_.add_scale_data(scale_data);
  return builder_.Finish();
}

inline flatbuffers::Offset<PPETask> CreatePPETaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<TensorReference> scale_data = 0,
    flatbuffers::Offset<PPEFixedFunction> fixed_function = 0,
    const std::vector<uint32_t> *instruction_list = nullptr) {
  auto instruction_list__ = instruction_list ? _fbb.CreateVector<uint32_t>(*instruction_list) : 0;
  return MVCNN::CreatePPETask(
      _fbb,
      scale_data,
      fixed_function,
      instruction_list__);
}

flatbuffers::Offset<PPETask> CreatePPETask(flatbuffers::FlatBufferBuilder &_fbb, const PPETaskT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NCEInvariantFieldsT : public flatbuffers::NativeTable {
  typedef NCEInvariantFields TableType;
  DPULayerType dpu_task_type;
  std::unique_ptr<PPETaskT> ppe_task;
  MPE_Mode mpe_frequent_mode;
  int16_t kernelH;
  int16_t kernelW;
  int16_t kernel_strideH;
  int16_t kernel_strideW;
  int16_t kernel_padLeft;
  int16_t kernel_padRight;
  int16_t kernel_padTop;
  int16_t kernel_padBottom;
  std::unique_ptr<TensorReferenceT> parent_input_tensor;
  std::unique_ptr<TensorReferenceT> parent_output_tensor;
  std::unique_ptr<TensorReferenceT> parent_weights_tensor;
  std::unique_ptr<TensorReferenceT> input_data;
  std::unique_ptr<TensorReferenceT> output_data;
  std::unique_ptr<TensorReferenceT> weights_data;
  std::unique_ptr<TensorReferenceT> weights_table;
  std::unique_ptr<TensorReferenceT> activation_window;
  int32_t activation_window_channel_length;
  std::vector<NN2Optimization> enabled_optimizations;
  int32_t odu_offset;
  int32_t out_channel_offset;
  NCEInvariantFieldsT()
      : dpu_task_type(DPULayerType_CONV),
        mpe_frequent_mode(MPE_Mode_VECTOR),
        kernelH(0),
        kernelW(0),
        kernel_strideH(0),
        kernel_strideW(0),
        kernel_padLeft(0),
        kernel_padRight(0),
        kernel_padTop(0),
        kernel_padBottom(0),
        activation_window_channel_length(0),
        odu_offset(0),
        out_channel_offset(0) {
  }
};

struct NCEInvariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NCEInvariantFieldsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_DPU_TASK_TYPE = 4,
    VT_PPE_TASK = 6,
    VT_MPE_FREQUENT_MODE = 10,
    VT_KERNELH = 12,
    VT_KERNELW = 14,
    VT_KERNEL_STRIDEH = 16,
    VT_KERNEL_STRIDEW = 18,
    VT_KERNEL_PADLEFT = 20,
    VT_KERNEL_PADRIGHT = 22,
    VT_KERNEL_PADTOP = 24,
    VT_KERNEL_PADBOTTOM = 26,
    VT_PARENT_INPUT_TENSOR = 28,
    VT_PARENT_OUTPUT_TENSOR = 30,
    VT_PARENT_WEIGHTS_TENSOR = 32,
    VT_INPUT_DATA = 34,
    VT_OUTPUT_DATA = 36,
    VT_WEIGHTS_DATA = 38,
    VT_WEIGHTS_TABLE = 40,
    VT_ACTIVATION_WINDOW = 42,
    VT_ACTIVATION_WINDOW_CHANNEL_LENGTH = 44,
    VT_ENABLED_OPTIMIZATIONS = 46,
    VT_ODU_OFFSET = 56,
    VT_OUT_CHANNEL_OFFSET = 58
  };
  /// This object describes the common information that any subtasks
  /// share. This is generally layer information.
  /// This object is focussed on sharable items for DPUTasks.
  ///
  /// Each NCE2Task consists of:
  /// - a single DPU operation
  /// - one or more PPE Tasks
  ///    Any additional PPE Tasks past the first task must be triggered by
  ///    A NN Shave task or other external mechanism
  /// - zero or more NN Shave Tasks
  ///    These can be triggered by the PPE Tasks.
  ///
  /// Both DPU and PPE have configurations to 'do nothing' if desired
  /// This must still be specified however.
  ///
  DPULayerType dpu_task_type() const {
    return static_cast<DPULayerType>(GetField<int8_t>(VT_DPU_TASK_TYPE, 0));
  }
  const PPETask *ppe_task() const {
    return GetPointer<const PPETask *>(VT_PPE_TASK);
  }
  MPE_Mode mpe_frequent_mode() const {
    return static_cast<MPE_Mode>(GetField<int8_t>(VT_MPE_FREQUENT_MODE, 0));
  }
  int16_t kernelH() const {
    return GetField<int16_t>(VT_KERNELH, 0);
  }
  int16_t kernelW() const {
    return GetField<int16_t>(VT_KERNELW, 0);
  }
  int16_t kernel_strideH() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEH, 0);
  }
  int16_t kernel_strideW() const {
    return GetField<int16_t>(VT_KERNEL_STRIDEW, 0);
  }
  int16_t kernel_padLeft() const {
    return GetField<int16_t>(VT_KERNEL_PADLEFT, 0);
  }
  int16_t kernel_padRight() const {
    return GetField<int16_t>(VT_KERNEL_PADRIGHT, 0);
  }
  int16_t kernel_padTop() const {
    return GetField<int16_t>(VT_KERNEL_PADTOP, 0);
  }
  int16_t kernel_padBottom() const {
    return GetField<int16_t>(VT_KERNEL_PADBOTTOM, 0);
  }
  const TensorReference *parent_input_tensor() const {
    return GetPointer<const TensorReference *>(VT_PARENT_INPUT_TENSOR);
  }
  const TensorReference *parent_output_tensor() const {
    return GetPointer<const TensorReference *>(VT_PARENT_OUTPUT_TENSOR);
  }
  const TensorReference *parent_weights_tensor() const {
    return GetPointer<const TensorReference *>(VT_PARENT_WEIGHTS_TENSOR);
  }
  const TensorReference *input_data() const {
    return GetPointer<const TensorReference *>(VT_INPUT_DATA);
  }
  const TensorReference *output_data() const {
    return GetPointer<const TensorReference *>(VT_OUTPUT_DATA);
  }
  const TensorReference *weights_data() const {
    return GetPointer<const TensorReference *>(VT_WEIGHTS_DATA);
  }
  const TensorReference *weights_table() const {
    return GetPointer<const TensorReference *>(VT_WEIGHTS_TABLE);
  }
  const TensorReference *activation_window() const {
    return GetPointer<const TensorReference *>(VT_ACTIVATION_WINDOW);
  }
  int32_t activation_window_channel_length() const {
    return GetField<int32_t>(VT_ACTIVATION_WINDOW_CHANNEL_LENGTH, 0);
  }
  const flatbuffers::Vector<int8_t> *enabled_optimizations() const {
    return GetPointer<const flatbuffers::Vector<int8_t> *>(VT_ENABLED_OPTIMIZATIONS);
  }
  int32_t odu_offset() const {
    return GetField<int32_t>(VT_ODU_OFFSET, 0);
  }
  int32_t out_channel_offset() const {
    return GetField<int32_t>(VT_OUT_CHANNEL_OFFSET, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyField<int8_t>(verifier, VT_DPU_TASK_TYPE) &&
           VerifyOffset(verifier, VT_PPE_TASK) &&
           verifier.VerifyTable(ppe_task()) &&
           VerifyField<int8_t>(verifier, VT_MPE_FREQUENT_MODE) &&
           VerifyField<int16_t>(verifier, VT_KERNELH) &&
           VerifyField<int16_t>(verifier, VT_KERNELW) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEH) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_STRIDEW) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_PADLEFT) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_PADRIGHT) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_PADTOP) &&
           VerifyField<int16_t>(verifier, VT_KERNEL_PADBOTTOM) &&
           VerifyOffset(verifier, VT_PARENT_INPUT_TENSOR) &&
           verifier.VerifyTable(parent_input_tensor()) &&
           VerifyOffset(verifier, VT_PARENT_OUTPUT_TENSOR) &&
           verifier.VerifyTable(parent_output_tensor()) &&
           VerifyOffset(verifier, VT_PARENT_WEIGHTS_TENSOR) &&
           verifier.VerifyTable(parent_weights_tensor()) &&
           VerifyOffset(verifier, VT_INPUT_DATA) &&
           verifier.VerifyTable(input_data()) &&
           VerifyOffset(verifier, VT_OUTPUT_DATA) &&
           verifier.VerifyTable(output_data()) &&
           VerifyOffset(verifier, VT_WEIGHTS_DATA) &&
           verifier.VerifyTable(weights_data()) &&
           VerifyOffset(verifier, VT_WEIGHTS_TABLE) &&
           verifier.VerifyTable(weights_table()) &&
           VerifyOffset(verifier, VT_ACTIVATION_WINDOW) &&
           verifier.VerifyTable(activation_window()) &&
           VerifyField<int32_t>(verifier, VT_ACTIVATION_WINDOW_CHANNEL_LENGTH) &&
           VerifyOffset(verifier, VT_ENABLED_OPTIMIZATIONS) &&
           verifier.VerifyVector(enabled_optimizations()) &&
           VerifyField<int32_t>(verifier, VT_ODU_OFFSET) &&
           VerifyField<int32_t>(verifier, VT_OUT_CHANNEL_OFFSET) &&
           verifier.EndTable();
  }
  NCEInvariantFieldsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NCEInvariantFieldsT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NCEInvariantFields> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCEInvariantFieldsT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NCEInvariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_dpu_task_type(DPULayerType dpu_task_type) {
    fbb_.AddElement<int8_t>(NCEInvariantFields::VT_DPU_TASK_TYPE, static_cast<int8_t>(dpu_task_type), 0);
  }
  void add_ppe_task(flatbuffers::Offset<PPETask> ppe_task) {
    fbb_.AddOffset(NCEInvariantFields::VT_PPE_TASK, ppe_task);
  }
  void add_mpe_frequent_mode(MPE_Mode mpe_frequent_mode) {
    fbb_.AddElement<int8_t>(NCEInvariantFields::VT_MPE_FREQUENT_MODE, static_cast<int8_t>(mpe_frequent_mode), 0);
  }
  void add_kernelH(int16_t kernelH) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNELH, kernelH, 0);
  }
  void add_kernelW(int16_t kernelW) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNELW, kernelW, 0);
  }
  void add_kernel_strideH(int16_t kernel_strideH) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_STRIDEH, kernel_strideH, 0);
  }
  void add_kernel_strideW(int16_t kernel_strideW) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_STRIDEW, kernel_strideW, 0);
  }
  void add_kernel_padLeft(int16_t kernel_padLeft) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_PADLEFT, kernel_padLeft, 0);
  }
  void add_kernel_padRight(int16_t kernel_padRight) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_PADRIGHT, kernel_padRight, 0);
  }
  void add_kernel_padTop(int16_t kernel_padTop) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_PADTOP, kernel_padTop, 0);
  }
  void add_kernel_padBottom(int16_t kernel_padBottom) {
    fbb_.AddElement<int16_t>(NCEInvariantFields::VT_KERNEL_PADBOTTOM, kernel_padBottom, 0);
  }
  void add_parent_input_tensor(flatbuffers::Offset<TensorReference> parent_input_tensor) {
    fbb_.AddOffset(NCEInvariantFields::VT_PARENT_INPUT_TENSOR, parent_input_tensor);
  }
  void add_parent_output_tensor(flatbuffers::Offset<TensorReference> parent_output_tensor) {
    fbb_.AddOffset(NCEInvariantFields::VT_PARENT_OUTPUT_TENSOR, parent_output_tensor);
  }
  void add_parent_weights_tensor(flatbuffers::Offset<TensorReference> parent_weights_tensor) {
    fbb_.AddOffset(NCEInvariantFields::VT_PARENT_WEIGHTS_TENSOR, parent_weights_tensor);
  }
  void add_input_data(flatbuffers::Offset<TensorReference> input_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_INPUT_DATA, input_data);
  }
  void add_output_data(flatbuffers::Offset<TensorReference> output_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_OUTPUT_DATA, output_data);
  }
  void add_weights_data(flatbuffers::Offset<TensorReference> weights_data) {
    fbb_.AddOffset(NCEInvariantFields::VT_WEIGHTS_DATA, weights_data);
  }
  void add_weights_table(flatbuffers::Offset<TensorReference> weights_table) {
    fbb_.AddOffset(NCEInvariantFields::VT_WEIGHTS_TABLE, weights_table);
  }
  void add_activation_window(flatbuffers::Offset<TensorReference> activation_window) {
    fbb_.AddOffset(NCEInvariantFields::VT_ACTIVATION_WINDOW, activation_window);
  }
  void add_activation_window_channel_length(int32_t activation_window_channel_length) {
    fbb_.AddElement<int32_t>(NCEInvariantFields::VT_ACTIVATION_WINDOW_CHANNEL_LENGTH, activation_window_channel_length, 0);
  }
  void add_enabled_optimizations(flatbuffers::Offset<flatbuffers::Vector<int8_t>> enabled_optimizations) {
    fbb_.AddOffset(NCEInvariantFields::VT_ENABLED_OPTIMIZATIONS, enabled_optimizations);
  }
  void add_odu_offset(int32_t odu_offset) {
    fbb_.AddElement<int32_t>(NCEInvariantFields::VT_ODU_OFFSET, odu_offset, 0);
  }
  void add_out_channel_offset(int32_t out_channel_offset) {
    fbb_.AddElement<int32_t>(NCEInvariantFields::VT_OUT_CHANNEL_OFFSET, out_channel_offset, 0);
  }
  explicit NCEInvariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCEInvariantFieldsBuilder &operator=(const NCEInvariantFieldsBuilder &);
  flatbuffers::Offset<NCEInvariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCEInvariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    DPULayerType dpu_task_type = DPULayerType_CONV,
    flatbuffers::Offset<PPETask> ppe_task = 0,
    MPE_Mode mpe_frequent_mode = MPE_Mode_VECTOR,
    int16_t kernelH = 0,
    int16_t kernelW = 0,
    int16_t kernel_strideH = 0,
    int16_t kernel_strideW = 0,
    int16_t kernel_padLeft = 0,
    int16_t kernel_padRight = 0,
    int16_t kernel_padTop = 0,
    int16_t kernel_padBottom = 0,
    flatbuffers::Offset<TensorReference> parent_input_tensor = 0,
    flatbuffers::Offset<TensorReference> parent_output_tensor = 0,
    flatbuffers::Offset<TensorReference> parent_weights_tensor = 0,
    flatbuffers::Offset<TensorReference> input_data = 0,
    flatbuffers::Offset<TensorReference> output_data = 0,
    flatbuffers::Offset<TensorReference> weights_data = 0,
    flatbuffers::Offset<TensorReference> weights_table = 0,
    flatbuffers::Offset<TensorReference> activation_window = 0,
    int32_t activation_window_channel_length = 0,
    flatbuffers::Offset<flatbuffers::Vector<int8_t>> enabled_optimizations = 0,
    int32_t odu_offset = 0,
    int32_t out_channel_offset = 0) {
  NCEInvariantFieldsBuilder builder_(_fbb);
  builder_.add_out_channel_offset(out_channel_offset);
  builder_.add_odu_offset(odu_offset);
  builder_.add_enabled_optimizations(enabled_optimizations);
  builder_.add_activation_window_channel_length(activation_window_channel_length);
  builder_.add_activation_window(activation_window);
  builder_.add_weights_table(weights_table);
  builder_.add_weights_data(weights_data);
  builder_.add_output_data(output_data);
  builder_.add_input_data(input_data);
  builder_.add_parent_weights_tensor(parent_weights_tensor);
  builder_.add_parent_output_tensor(parent_output_tensor);
  builder_.add_parent_input_tensor(parent_input_tensor);
  builder_.add_ppe_task(ppe_task);
  builder_.add_kernel_padBottom(kernel_padBottom);
  builder_.add_kernel_padTop(kernel_padTop);
  builder_.add_kernel_padRight(kernel_padRight);
  builder_.add_kernel_padLeft(kernel_padLeft);
  builder_.add_kernel_strideW(kernel_strideW);
  builder_.add_kernel_strideH(kernel_strideH);
  builder_.add_kernelW(kernelW);
  builder_.add_kernelH(kernelH);
  builder_.add_mpe_frequent_mode(mpe_frequent_mode);
  builder_.add_dpu_task_type(dpu_task_type);
  return builder_.Finish();
}

inline flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFieldsDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    DPULayerType dpu_task_type = DPULayerType_CONV,
    flatbuffers::Offset<PPETask> ppe_task = 0,
    MPE_Mode mpe_frequent_mode = MPE_Mode_VECTOR,
    int16_t kernelH = 0,
    int16_t kernelW = 0,
    int16_t kernel_strideH = 0,
    int16_t kernel_strideW = 0,
    int16_t kernel_padLeft = 0,
    int16_t kernel_padRight = 0,
    int16_t kernel_padTop = 0,
    int16_t kernel_padBottom = 0,
    flatbuffers::Offset<TensorReference> parent_input_tensor = 0,
    flatbuffers::Offset<TensorReference> parent_output_tensor = 0,
    flatbuffers::Offset<TensorReference> parent_weights_tensor = 0,
    flatbuffers::Offset<TensorReference> input_data = 0,
    flatbuffers::Offset<TensorReference> output_data = 0,
    flatbuffers::Offset<TensorReference> weights_data = 0,
    flatbuffers::Offset<TensorReference> weights_table = 0,
    flatbuffers::Offset<TensorReference> activation_window = 0,
    int32_t activation_window_channel_length = 0,
    const std::vector<int8_t> *enabled_optimizations = nullptr,
    int32_t odu_offset = 0,
    int32_t out_channel_offset = 0) {
  auto enabled_optimizations__ = enabled_optimizations ? _fbb.CreateVector<int8_t>(*enabled_optimizations) : 0;
  return MVCNN::CreateNCEInvariantFields(
      _fbb,
      dpu_task_type,
      ppe_task,
      mpe_frequent_mode,
      kernelH,
      kernelW,
      kernel_strideH,
      kernel_strideW,
      kernel_padLeft,
      kernel_padRight,
      kernel_padTop,
      kernel_padBottom,
      parent_input_tensor,
      parent_output_tensor,
      parent_weights_tensor,
      input_data,
      output_data,
      weights_data,
      weights_table,
      activation_window,
      activation_window_channel_length,
      enabled_optimizations__,
      odu_offset,
      out_channel_offset);
}

flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFields(flatbuffers::FlatBufferBuilder &_fbb, const NCEInvariantFieldsT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NCEVariantFieldsT : public flatbuffers::NativeTable {
  typedef NCEVariantFields TableType;
  std::unique_ptr<BarrierReferenceT> associated_barriers;
  MPE_Mode mpe_mode;
  int16_t padLeft;
  int16_t padRight;
  int16_t padTop;
  int16_t padBottom;
  int16_t workload_start_X;
  int16_t workload_start_Y;
  int16_t workload_start_Z;
  int16_t workload_end_X;
  int16_t workload_end_Y;
  int16_t workload_end_Z;
  NCEVariantFieldsT()
      : mpe_mode(MPE_Mode_VECTOR),
        padLeft(0),
        padRight(0),
        padTop(0),
        padBottom(0),
        workload_start_X(0),
        workload_start_Y(0),
        workload_start_Z(0),
        workload_end_X(0),
        workload_end_Y(0),
        workload_end_Z(0) {
  }
};

struct NCEVariantFields FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NCEVariantFieldsT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_ASSOCIATED_BARRIERS = 4,
    VT_MPE_MODE = 6,
    VT_PADLEFT = 8,
    VT_PADRIGHT = 10,
    VT_PADTOP = 12,
    VT_PADBOTTOM = 14,
    VT_WORKLOAD_START_X = 16,
    VT_WORKLOAD_START_Y = 18,
    VT_WORKLOAD_START_Z = 20,
    VT_WORKLOAD_END_X = 22,
    VT_WORKLOAD_END_Y = 24,
    VT_WORKLOAD_END_Z = 26
  };
  /// This object describes the information that any subtasks
  /// vary on. This is generally resource and position information.
  const BarrierReference *associated_barriers() const {
    return GetPointer<const BarrierReference *>(VT_ASSOCIATED_BARRIERS);
  }
  MPE_Mode mpe_mode() const {
    return static_cast<MPE_Mode>(GetField<int8_t>(VT_MPE_MODE, 0));
  }
  int16_t padLeft() const {
    return GetField<int16_t>(VT_PADLEFT, 0);
  }
  int16_t padRight() const {
    return GetField<int16_t>(VT_PADRIGHT, 0);
  }
  int16_t padTop() const {
    return GetField<int16_t>(VT_PADTOP, 0);
  }
  int16_t padBottom() const {
    return GetField<int16_t>(VT_PADBOTTOM, 0);
  }
  /// Co-Ordinates for the starting and ending location of this workload.
  /// Internally, the hardware will increment in steps of MPE_MODE's size
  /// from "start" until it reaches "end".
  int16_t workload_start_X() const {
    return GetField<int16_t>(VT_WORKLOAD_START_X, 0);
  }
  int16_t workload_start_Y() const {
    return GetField<int16_t>(VT_WORKLOAD_START_Y, 0);
  }
  int16_t workload_start_Z() const {
    return GetField<int16_t>(VT_WORKLOAD_START_Z, 0);
  }
  int16_t workload_end_X() const {
    return GetField<int16_t>(VT_WORKLOAD_END_X, 0);
  }
  int16_t workload_end_Y() const {
    return GetField<int16_t>(VT_WORKLOAD_END_Y, 0);
  }
  int16_t workload_end_Z() const {
    return GetField<int16_t>(VT_WORKLOAD_END_Z, 0);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_ASSOCIATED_BARRIERS) &&
           verifier.VerifyTable(associated_barriers()) &&
           VerifyField<int8_t>(verifier, VT_MPE_MODE) &&
           VerifyField<int16_t>(verifier, VT_PADLEFT) &&
           VerifyField<int16_t>(verifier, VT_PADRIGHT) &&
           VerifyField<int16_t>(verifier, VT_PADTOP) &&
           VerifyField<int16_t>(verifier, VT_PADBOTTOM) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_X) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_Y) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_START_Z) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_X) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_Y) &&
           VerifyField<int16_t>(verifier, VT_WORKLOAD_END_Z) &&
           verifier.EndTable();
  }
  NCEVariantFieldsT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NCEVariantFieldsT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NCEVariantFields> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCEVariantFieldsT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NCEVariantFieldsBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_associated_barriers(flatbuffers::Offset<BarrierReference> associated_barriers) {
    fbb_.AddOffset(NCEVariantFields::VT_ASSOCIATED_BARRIERS, associated_barriers);
  }
  void add_mpe_mode(MPE_Mode mpe_mode) {
    fbb_.AddElement<int8_t>(NCEVariantFields::VT_MPE_MODE, static_cast<int8_t>(mpe_mode), 0);
  }
  void add_padLeft(int16_t padLeft) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADLEFT, padLeft, 0);
  }
  void add_padRight(int16_t padRight) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADRIGHT, padRight, 0);
  }
  void add_padTop(int16_t padTop) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADTOP, padTop, 0);
  }
  void add_padBottom(int16_t padBottom) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_PADBOTTOM, padBottom, 0);
  }
  void add_workload_start_X(int16_t workload_start_X) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_X, workload_start_X, 0);
  }
  void add_workload_start_Y(int16_t workload_start_Y) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_Y, workload_start_Y, 0);
  }
  void add_workload_start_Z(int16_t workload_start_Z) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_START_Z, workload_start_Z, 0);
  }
  void add_workload_end_X(int16_t workload_end_X) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_X, workload_end_X, 0);
  }
  void add_workload_end_Y(int16_t workload_end_Y) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_Y, workload_end_Y, 0);
  }
  void add_workload_end_Z(int16_t workload_end_Z) {
    fbb_.AddElement<int16_t>(NCEVariantFields::VT_WORKLOAD_END_Z, workload_end_Z, 0);
  }
  explicit NCEVariantFieldsBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCEVariantFieldsBuilder &operator=(const NCEVariantFieldsBuilder &);
  flatbuffers::Offset<NCEVariantFields> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCEVariantFields>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCEVariantFields> CreateNCEVariantFields(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<BarrierReference> associated_barriers = 0,
    MPE_Mode mpe_mode = MPE_Mode_VECTOR,
    int16_t padLeft = 0,
    int16_t padRight = 0,
    int16_t padTop = 0,
    int16_t padBottom = 0,
    int16_t workload_start_X = 0,
    int16_t workload_start_Y = 0,
    int16_t workload_start_Z = 0,
    int16_t workload_end_X = 0,
    int16_t workload_end_Y = 0,
    int16_t workload_end_Z = 0) {
  NCEVariantFieldsBuilder builder_(_fbb);
  builder_.add_associated_barriers(associated_barriers);
  builder_.add_workload_end_Z(workload_end_Z);
  builder_.add_workload_end_Y(workload_end_Y);
  builder_.add_workload_end_X(workload_end_X);
  builder_.add_workload_start_Z(workload_start_Z);
  builder_.add_workload_start_Y(workload_start_Y);
  builder_.add_workload_start_X(workload_start_X);
  builder_.add_padBottom(padBottom);
  builder_.add_padTop(padTop);
  builder_.add_padRight(padRight);
  builder_.add_padLeft(padLeft);
  builder_.add_mpe_mode(mpe_mode);
  return builder_.Finish();
}

flatbuffers::Offset<NCEVariantFields> CreateNCEVariantFields(flatbuffers::FlatBufferBuilder &_fbb, const NCEVariantFieldsT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

struct NCE2TaskT : public flatbuffers::NativeTable {
  typedef NCE2Task TableType;
  std::unique_ptr<NCEInvariantFieldsT> invariant;
  std::vector<std::unique_ptr<NCEVariantFieldsT>> variant;
  NCE2TaskT() {
  }
};

struct NCE2Task FLATBUFFERS_FINAL_CLASS : private flatbuffers::Table {
  typedef NCE2TaskT NativeTableType;
  enum FlatBuffersVTableOffset FLATBUFFERS_VTABLE_UNDERLYING_TYPE {
    VT_INVARIANT = 4,
    VT_VARIANT = 6
  };
  /// This object describes an entire network layer that will be operated
  /// on by the NCE's DPUs, PPEs and NNSHV Assist Library.
  ///
  /// The layer is likely to be split into different "workloads" -
  /// subsections of the layer for the processors to split work upon.
  ///
  /// Fields common to these subsections are to be stored in the 'invariant'
  /// part of this object.
  /// All per-section information should be contained in the 'variant'
  /// vector. Where there is one entry per 'workload'. If there are no unique
  /// information per-workload, empty objects should still be placed.
  /// There is a 1-to-1 Relationship between DPUs and "Workloads"
  ///
  /// Below is a typical example of splitting a Convolution
  /// across 5 DPUs (1 Cluster)
  ///                                XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
  ///                         XXXXXXX          XXXXX              XXXXXX
  ///                     XXXXX            XXXXX               XXX     X
  ///                XXXXX             XXXXX                XXXX       X
  ///         XXXXXXXX             XXXXX               XXXXX           X
  ///    XXXXXX                XXXXX             X XX X               XX
  /// XX-------------------+XXX----------------+XX                 XXXXX
  /// |                    |                   |               XXXX    X
  /// |                    |                   |           XXXXX       X
  /// |                    |        C          |      XXXXXX           X
  /// |         A          |                   |  XXXX               XXX
  /// |                    +-------------------+XXX               XXXX X
  /// |                    |                   |               XXXX    X
  /// |                    |                   |            XXXX       X
  /// +--------------------+        D          |        XXXXX          X
  /// |                    |                   |    XXXXX           X XX
  /// |                    |                   | XXX             XXXX
  /// |                    +-------------------XX             XXXX
  /// |         B          |                   |           XXXX
  /// |                    |                   |        XXXX
  /// |                    |        E          |      XXX
  /// |                    |                   |   XX
  /// +--------------------+-------------------+XX
  ///
  /// Splits for workloads are not limited to different dimensions when using a
  /// Single Cluster.
  /// However, splitting across clusters is limited to splitting over height
  /// and splitting over channels.
  const NCEInvariantFields *invariant() const {
    return GetPointer<const NCEInvariantFields *>(VT_INVARIANT);
  }
  const flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>> *variant() const {
    return GetPointer<const flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>> *>(VT_VARIANT);
  }
  bool Verify(flatbuffers::Verifier &verifier) const {
    return VerifyTableStart(verifier) &&
           VerifyOffset(verifier, VT_INVARIANT) &&
           verifier.VerifyTable(invariant()) &&
           VerifyOffset(verifier, VT_VARIANT) &&
           verifier.VerifyVector(variant()) &&
           verifier.VerifyVectorOfTables(variant()) &&
           verifier.EndTable();
  }
  NCE2TaskT *UnPack(const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  void UnPackTo(NCE2TaskT *_o, const flatbuffers::resolver_function_t *_resolver = nullptr) const;
  static flatbuffers::Offset<NCE2Task> Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCE2TaskT* _o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);
};

struct NCE2TaskBuilder {
  flatbuffers::FlatBufferBuilder &fbb_;
  flatbuffers::uoffset_t start_;
  void add_invariant(flatbuffers::Offset<NCEInvariantFields> invariant) {
    fbb_.AddOffset(NCE2Task::VT_INVARIANT, invariant);
  }
  void add_variant(flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>>> variant) {
    fbb_.AddOffset(NCE2Task::VT_VARIANT, variant);
  }
  explicit NCE2TaskBuilder(flatbuffers::FlatBufferBuilder &_fbb)
        : fbb_(_fbb) {
    start_ = fbb_.StartTable();
  }
  NCE2TaskBuilder &operator=(const NCE2TaskBuilder &);
  flatbuffers::Offset<NCE2Task> Finish() {
    const auto end = fbb_.EndTable(start_);
    auto o = flatbuffers::Offset<NCE2Task>(end);
    return o;
  }
};

inline flatbuffers::Offset<NCE2Task> CreateNCE2Task(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<NCEInvariantFields> invariant = 0,
    flatbuffers::Offset<flatbuffers::Vector<flatbuffers::Offset<NCEVariantFields>>> variant = 0) {
  NCE2TaskBuilder builder_(_fbb);
  builder_.add_variant(variant);
  builder_.add_invariant(invariant);
  return builder_.Finish();
}

inline flatbuffers::Offset<NCE2Task> CreateNCE2TaskDirect(
    flatbuffers::FlatBufferBuilder &_fbb,
    flatbuffers::Offset<NCEInvariantFields> invariant = 0,
    const std::vector<flatbuffers::Offset<NCEVariantFields>> *variant = nullptr) {
  auto variant__ = variant ? _fbb.CreateVector<flatbuffers::Offset<NCEVariantFields>>(*variant) : 0;
  return MVCNN::CreateNCE2Task(
      _fbb,
      invariant,
      variant__);
}

flatbuffers::Offset<NCE2Task> CreateNCE2Task(flatbuffers::FlatBufferBuilder &_fbb, const NCE2TaskT *_o, const flatbuffers::rehasher_function_t *_rehasher = nullptr);

inline PPEFixedFunctionT *PPEFixedFunction::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new PPEFixedFunctionT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void PPEFixedFunction::UnPackTo(PPEFixedFunctionT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = Ops(); if (_e) { _o->Ops.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->Ops[_i] = static_cast<PPELayerType>(_e->Get(_i)); } } };
  { auto _e = Clamp_Low(); _o->Clamp_Low = _e; };
  { auto _e = Clamp_High(); _o->Clamp_High = _e; };
  { auto _e = Lrelu_Mult(); _o->Lrelu_Mult = _e; };
  { auto _e = Lrelu_Shift(); _o->Lrelu_Shift = _e; };
}

inline flatbuffers::Offset<PPEFixedFunction> PPEFixedFunction::Pack(flatbuffers::FlatBufferBuilder &_fbb, const PPEFixedFunctionT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePPEFixedFunction(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<PPEFixedFunction> CreatePPEFixedFunction(flatbuffers::FlatBufferBuilder &_fbb, const PPEFixedFunctionT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PPEFixedFunctionT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _Ops = _fbb.CreateVectorScalarCast<uint8_t>(flatbuffers::data(_o->Ops), _o->Ops.size());
  auto _Clamp_Low = _o->Clamp_Low;
  auto _Clamp_High = _o->Clamp_High;
  auto _Lrelu_Mult = _o->Lrelu_Mult;
  auto _Lrelu_Shift = _o->Lrelu_Shift;
  return MVCNN::CreatePPEFixedFunction(
      _fbb,
      _Ops,
      _Clamp_Low,
      _Clamp_High,
      _Lrelu_Mult,
      _Lrelu_Shift);
}

inline PPETaskT *PPETask::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new PPETaskT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void PPETask::UnPackTo(PPETaskT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = scale_data(); if (_e) _o->scale_data = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = fixed_function(); if (_e) _o->fixed_function = std::unique_ptr<PPEFixedFunctionT>(_e->UnPack(_resolver)); };
  { auto _e = instruction_list(); if (_e) { _o->instruction_list.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->instruction_list[_i] = _e->Get(_i); } } };
}

inline flatbuffers::Offset<PPETask> PPETask::Pack(flatbuffers::FlatBufferBuilder &_fbb, const PPETaskT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreatePPETask(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<PPETask> CreatePPETask(flatbuffers::FlatBufferBuilder &_fbb, const PPETaskT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const PPETaskT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _scale_data = _o->scale_data ? CreateTensorReference(_fbb, _o->scale_data.get(), _rehasher) : 0;
  auto _fixed_function = _o->fixed_function ? CreatePPEFixedFunction(_fbb, _o->fixed_function.get(), _rehasher) : 0;
  auto _instruction_list = _fbb.CreateVector(_o->instruction_list);
  return MVCNN::CreatePPETask(
      _fbb,
      _scale_data,
      _fixed_function,
      _instruction_list);
}

inline NCEInvariantFieldsT *NCEInvariantFields::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NCEInvariantFieldsT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NCEInvariantFields::UnPackTo(NCEInvariantFieldsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = dpu_task_type(); _o->dpu_task_type = _e; };
  { auto _e = ppe_task(); if (_e) _o->ppe_task = std::unique_ptr<PPETaskT>(_e->UnPack(_resolver)); };
  { auto _e = mpe_frequent_mode(); _o->mpe_frequent_mode = _e; };
  { auto _e = kernelH(); _o->kernelH = _e; };
  { auto _e = kernelW(); _o->kernelW = _e; };
  { auto _e = kernel_strideH(); _o->kernel_strideH = _e; };
  { auto _e = kernel_strideW(); _o->kernel_strideW = _e; };
  { auto _e = kernel_padLeft(); _o->kernel_padLeft = _e; };
  { auto _e = kernel_padRight(); _o->kernel_padRight = _e; };
  { auto _e = kernel_padTop(); _o->kernel_padTop = _e; };
  { auto _e = kernel_padBottom(); _o->kernel_padBottom = _e; };
  { auto _e = parent_input_tensor(); if (_e) _o->parent_input_tensor = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = parent_output_tensor(); if (_e) _o->parent_output_tensor = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = parent_weights_tensor(); if (_e) _o->parent_weights_tensor = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = input_data(); if (_e) _o->input_data = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = output_data(); if (_e) _o->output_data = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = weights_data(); if (_e) _o->weights_data = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = weights_table(); if (_e) _o->weights_table = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = activation_window(); if (_e) _o->activation_window = std::unique_ptr<TensorReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = activation_window_channel_length(); _o->activation_window_channel_length = _e; };
  { auto _e = enabled_optimizations(); if (_e) { _o->enabled_optimizations.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->enabled_optimizations[_i] = static_cast<NN2Optimization>(_e->Get(_i)); } } };
  { auto _e = odu_offset(); _o->odu_offset = _e; };
  { auto _e = out_channel_offset(); _o->out_channel_offset = _e; };
}

inline flatbuffers::Offset<NCEInvariantFields> NCEInvariantFields::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCEInvariantFieldsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNCEInvariantFields(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NCEInvariantFields> CreateNCEInvariantFields(flatbuffers::FlatBufferBuilder &_fbb, const NCEInvariantFieldsT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NCEInvariantFieldsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _dpu_task_type = _o->dpu_task_type;
  auto _ppe_task = _o->ppe_task ? CreatePPETask(_fbb, _o->ppe_task.get(), _rehasher) : 0;
  auto _mpe_frequent_mode = _o->mpe_frequent_mode;
  auto _kernelH = _o->kernelH;
  auto _kernelW = _o->kernelW;
  auto _kernel_strideH = _o->kernel_strideH;
  auto _kernel_strideW = _o->kernel_strideW;
  auto _kernel_padLeft = _o->kernel_padLeft;
  auto _kernel_padRight = _o->kernel_padRight;
  auto _kernel_padTop = _o->kernel_padTop;
  auto _kernel_padBottom = _o->kernel_padBottom;
  auto _parent_input_tensor = _o->parent_input_tensor ? CreateTensorReference(_fbb, _o->parent_input_tensor.get(), _rehasher) : 0;
  auto _parent_output_tensor = _o->parent_output_tensor ? CreateTensorReference(_fbb, _o->parent_output_tensor.get(), _rehasher) : 0;
  auto _parent_weights_tensor = _o->parent_weights_tensor ? CreateTensorReference(_fbb, _o->parent_weights_tensor.get(), _rehasher) : 0;
  auto _input_data = _o->input_data ? CreateTensorReference(_fbb, _o->input_data.get(), _rehasher) : 0;
  auto _output_data = _o->output_data ? CreateTensorReference(_fbb, _o->output_data.get(), _rehasher) : 0;
  auto _weights_data = _o->weights_data ? CreateTensorReference(_fbb, _o->weights_data.get(), _rehasher) : 0;
  auto _weights_table = _o->weights_table ? CreateTensorReference(_fbb, _o->weights_table.get(), _rehasher) : 0;
  auto _activation_window = _o->activation_window ? CreateTensorReference(_fbb, _o->activation_window.get(), _rehasher) : 0;
  auto _activation_window_channel_length = _o->activation_window_channel_length;
  auto _enabled_optimizations = _fbb.CreateVectorScalarCast<int8_t>(flatbuffers::data(_o->enabled_optimizations), _o->enabled_optimizations.size());
  auto _odu_offset = _o->odu_offset;
  auto _out_channel_offset = _o->out_channel_offset;
  return MVCNN::CreateNCEInvariantFields(
      _fbb,
      _dpu_task_type,
      _ppe_task,
      _mpe_frequent_mode,
      _kernelH,
      _kernelW,
      _kernel_strideH,
      _kernel_strideW,
      _kernel_padLeft,
      _kernel_padRight,
      _kernel_padTop,
      _kernel_padBottom,
      _parent_input_tensor,
      _parent_output_tensor,
      _parent_weights_tensor,
      _input_data,
      _output_data,
      _weights_data,
      _weights_table,
      _activation_window,
      _activation_window_channel_length,
      _enabled_optimizations,
      _odu_offset,
      _out_channel_offset);
}

inline NCEVariantFieldsT *NCEVariantFields::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NCEVariantFieldsT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NCEVariantFields::UnPackTo(NCEVariantFieldsT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = associated_barriers(); if (_e) _o->associated_barriers = std::unique_ptr<BarrierReferenceT>(_e->UnPack(_resolver)); };
  { auto _e = mpe_mode(); _o->mpe_mode = _e; };
  { auto _e = padLeft(); _o->padLeft = _e; };
  { auto _e = padRight(); _o->padRight = _e; };
  { auto _e = padTop(); _o->padTop = _e; };
  { auto _e = padBottom(); _o->padBottom = _e; };
  { auto _e = workload_start_X(); _o->workload_start_X = _e; };
  { auto _e = workload_start_Y(); _o->workload_start_Y = _e; };
  { auto _e = workload_start_Z(); _o->workload_start_Z = _e; };
  { auto _e = workload_end_X(); _o->workload_end_X = _e; };
  { auto _e = workload_end_Y(); _o->workload_end_Y = _e; };
  { auto _e = workload_end_Z(); _o->workload_end_Z = _e; };
}

inline flatbuffers::Offset<NCEVariantFields> NCEVariantFields::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCEVariantFieldsT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNCEVariantFields(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NCEVariantFields> CreateNCEVariantFields(flatbuffers::FlatBufferBuilder &_fbb, const NCEVariantFieldsT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NCEVariantFieldsT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _associated_barriers = _o->associated_barriers ? CreateBarrierReference(_fbb, _o->associated_barriers.get(), _rehasher) : 0;
  auto _mpe_mode = _o->mpe_mode;
  auto _padLeft = _o->padLeft;
  auto _padRight = _o->padRight;
  auto _padTop = _o->padTop;
  auto _padBottom = _o->padBottom;
  auto _workload_start_X = _o->workload_start_X;
  auto _workload_start_Y = _o->workload_start_Y;
  auto _workload_start_Z = _o->workload_start_Z;
  auto _workload_end_X = _o->workload_end_X;
  auto _workload_end_Y = _o->workload_end_Y;
  auto _workload_end_Z = _o->workload_end_Z;
  return MVCNN::CreateNCEVariantFields(
      _fbb,
      _associated_barriers,
      _mpe_mode,
      _padLeft,
      _padRight,
      _padTop,
      _padBottom,
      _workload_start_X,
      _workload_start_Y,
      _workload_start_Z,
      _workload_end_X,
      _workload_end_Y,
      _workload_end_Z);
}

inline NCE2TaskT *NCE2Task::UnPack(const flatbuffers::resolver_function_t *_resolver) const {
  auto _o = new NCE2TaskT();
  UnPackTo(_o, _resolver);
  return _o;
}

inline void NCE2Task::UnPackTo(NCE2TaskT *_o, const flatbuffers::resolver_function_t *_resolver) const {
  (void)_o;
  (void)_resolver;
  { auto _e = invariant(); if (_e) _o->invariant = std::unique_ptr<NCEInvariantFieldsT>(_e->UnPack(_resolver)); };
  { auto _e = variant(); if (_e) { _o->variant.resize(_e->size()); for (flatbuffers::uoffset_t _i = 0; _i < _e->size(); _i++) { _o->variant[_i] = std::unique_ptr<NCEVariantFieldsT>(_e->Get(_i)->UnPack(_resolver)); } } };
}

inline flatbuffers::Offset<NCE2Task> NCE2Task::Pack(flatbuffers::FlatBufferBuilder &_fbb, const NCE2TaskT* _o, const flatbuffers::rehasher_function_t *_rehasher) {
  return CreateNCE2Task(_fbb, _o, _rehasher);
}

inline flatbuffers::Offset<NCE2Task> CreateNCE2Task(flatbuffers::FlatBufferBuilder &_fbb, const NCE2TaskT *_o, const flatbuffers::rehasher_function_t *_rehasher) {
  (void)_rehasher;
  (void)_o;
  struct _VectorArgs { flatbuffers::FlatBufferBuilder *__fbb; const NCE2TaskT* __o; const flatbuffers::rehasher_function_t *__rehasher; } _va = { &_fbb, _o, _rehasher}; (void)_va;
  auto _invariant = _o->invariant ? CreateNCEInvariantFields(_fbb, _o->invariant.get(), _rehasher) : 0;
  auto _variant = _fbb.CreateVector<flatbuffers::Offset<NCEVariantFields>> (_o->variant.size(), [](size_t i, _VectorArgs *__va) { return CreateNCEVariantFields(*__va->__fbb, __va->__o->variant[i].get(), __va->__rehasher); }, &_va );
  return MVCNN::CreateNCE2Task(
      _fbb,
      _invariant,
      _variant);
}

}  // namespace MVCNN

#endif  // FLATBUFFERS_GENERATED_NNNCE2_MVCNN_H_
