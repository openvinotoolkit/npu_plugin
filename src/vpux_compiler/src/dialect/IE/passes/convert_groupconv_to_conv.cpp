//
// Copyright (C) 2023 Intel Corporation.
// SPDX-License-Identifier: Apache 2.0
//

#include "vpux/compiler/dialect/IE/ops.hpp"
#include "vpux/compiler/dialect/IE/passes.hpp"
#include "vpux/compiler/dialect/IE/utils/convolution_utils.hpp"
#include "vpux/compiler/dialect/VPU/utils/const_utils.hpp"
#include "vpux/compiler/utils/attributes.hpp"
#include "vpux/compiler/utils/rewriter.hpp"

#include <mlir/Transforms/DialectConversion.h>

using namespace vpux;

namespace {

//
// ConvertGroupConvToConvPass
//

class ConvertGroupConvToConvPass final : public IE::ConvertGroupConvToConvBase<ConvertGroupConvToConvPass> {
public:
    explicit ConvertGroupConvToConvPass(Logger log) {
        Base::initLogger(log, Base::getArgumentName());
    }

public:
    class GroupConvToSingleConvConverter;
    class GroupConvToMultiConvConverter;

private:
    void safeRunOnFunc() final;
};

//
// GroupConvToSingleConvConverter
//

class ConvertGroupConvToConvPass::GroupConvToSingleConvConverter final :
        public mlir::OpRewritePattern<IE::GroupConvolutionOp> {
public:
    GroupConvToSingleConvConverter(mlir::MLIRContext* ctx, mlir::PatternBenefit benefit, Logger log)
            : mlir::OpRewritePattern<IE::GroupConvolutionOp>(ctx, benefit), _log(log) {
    }

public:
    mlir::LogicalResult matchAndRewrite(IE::GroupConvolutionOp origOp, mlir::PatternRewriter& rewriter) const final;

private:
    Logger _log;
};

std::optional<int64_t> getZeroPoint(IE::FakeQuantizeOp fqOp) {
    const auto realType = fqOp.getInput().getType().cast<vpux::NDTypeInterface>();
    const auto realElemType = realType.getElementType().cast<mlir::FloatType>();
    auto inLowConst = fqOp.getInputLow().getDefiningOp<Const::DeclareOp>();
    auto inHighConst = fqOp.getInputHigh().getDefiningOp<Const::DeclareOp>();
    VPUX_THROW_UNLESS(inLowConst != nullptr && inHighConst != nullptr,
                      "Cannot get low and high constant of FakeQuantizeOp {0}", fqOp->getLoc());
    const auto quantizeElemType =
            getQuantizedType(inLowConst.getContentAttr(), inHighConst.getContentAttr(), fqOp.getLevels(), realElemType,
                             VPU::hasNegativeValues(inLowConst.getContent()), fqOp.getLoc(), fqOp.getAutoBroadcast());

    if (auto uniformQuantType = quantizeElemType.dyn_cast_or_null<mlir::quant::UniformQuantizedType>()) {
        return uniformQuantType.getZeroPoint();
    } else if (auto uniformQuantPerAxisType =
                       quantizeElemType.dyn_cast_or_null<mlir::quant::UniformQuantizedPerAxisType>()) {
        auto zeroPoints = uniformQuantPerAxisType.getZeroPoints();
        const auto isSameZeroPoint =
                std::adjacent_find(zeroPoints.begin(), zeroPoints.end(), std::not_equal_to<>()) == zeroPoints.end();

        if (isSameZeroPoint) {
            return zeroPoints.front();
        }
    }

    return std::nullopt;
}

mlir::Value createConstantOpForPadding(ShapeRef padShape, mlir::Type elemType, const int64_t padValue,
                                       mlir::PatternRewriter& rewriter, mlir::Location loc) {
    const auto dataStorageType = mlir::RankedTensorType::get(padShape.raw(), elemType);

    auto getDenseElementsAttr = [&]() {
        if (elemType.isF16()) {
            return mlir::DenseElementsAttr::get(dataStorageType, static_cast<float16>(padValue));
        } else if (elemType.isF32()) {
            return mlir::DenseElementsAttr::get(dataStorageType, static_cast<float>(padValue));
        }
        VPUX_THROW("UnSupported element data type {0}", elemType);
    };

    return rewriter.create<Const::DeclareOp>(loc, dataStorageType, Const::ContentAttr::get(getDenseElementsAttr()));
}

mlir::LogicalResult ConvertGroupConvToConvPass::GroupConvToSingleConvConverter::matchAndRewrite(
        IE::GroupConvolutionOp origOp, mlir::PatternRewriter& rewriter) const {
    _log.trace("Got GroupConvolutionOp layer at '{0}'", origOp->getLoc());
    VPUX_THROW_UNLESS(origOp.getType().getRank() == 4, "The pass currently can only support 4D input");

    const auto weights = origOp.getFilter();
    const auto weightsShape = weights.getType().cast<vpux::NDTypeInterface>().getShape();
    const auto groupNumb = origOp.getGroups().value();

    const auto groupInSize = weightsShape[Dims4D::Filter::IC];
    const auto groupOutSize = weightsShape[Dims4D::Filter::OC] / groupNumb;
    const auto groupInChannel = getShape(origOp.getInput())[Dims4D::Act::C] / groupNumb;
    const auto groupOutChannel = getShape(origOp.getOutput())[Dims4D::Act::C] / groupNumb;
    VPUX_THROW_UNLESS(groupInSize == groupInChannel && groupOutSize == groupOutChannel,
                      "groupInSize '{0}' not equal with input channel '{1}' or groupOutSize '{2}' not equal with "
                      "output channel '{3}' ",
                      groupInSize, groupInChannel, groupOutSize, groupOutChannel);

    auto weightsCst = weights.getDefiningOp<Const::DeclareOp>();
    auto weightsFQ = weights.getDefiningOp<IE::FakeQuantizeOp>();
    auto isWeightsHasFQ = false;
    int64_t padValue = 0;
    if (weightsFQ) {
        weightsCst = weightsFQ.getInput().getDefiningOp<Const::DeclareOp>();
        isWeightsHasFQ = true;

        auto potentialZeroPoint = getZeroPoint(weightsFQ);
        if (!potentialZeroPoint.has_value()) {
            _log.trace("Cannot get zero point or not all zero points are equal");
            return mlir::failure();
        }
        padValue = potentialZeroPoint.value();
    }

    if (weightsCst == nullptr) {
        _log.trace("Cannot get GroupConvolutionOp Weights at '{0}'", origOp->getLoc());
        return mlir::failure();
    }

    const auto weightsElemType = weightsCst.getOutput().getType().cast<vpux::NDTypeInterface>().getElementType();
    if (!weightsElemType.isF16() && !weightsElemType.isF32()) {
        _log.trace("Weights constant output type should be float16 or float32, but got '{0}'", weightsElemType);
        return mlir::failure();
    }

    const auto weightsContentAttr = weightsCst.getContentAttr();
    auto reconstructGroupWeights = [&](const int64_t groupIdx) -> mlir::Value {
        const auto subviewOffsets = Shape{(groupIdx - 1) * groupOutSize, 0, 0, 0};
        const auto subviewStaticShape = Shape{groupOutSize, weightsShape[Dims4D::Filter::IC],
                                              weightsShape[Dims4D::Filter::KY], weightsShape[Dims4D::Filter::KX]};
        auto groupWeights = weightsContentAttr.subview(subviewOffsets, subviewStaticShape);

        if (isWeightsHasFQ) {
            SmallVector<mlir::Value> concatInputs;
            if (groupIdx > 1) {
                const auto padBefore = Shape{groupOutSize, (groupIdx - 1) * groupInSize,
                                             weightsShape[Dims4D::Filter::KY], weightsShape[Dims4D::Filter::KX]};
                concatInputs.push_back(createConstantOpForPadding(padBefore, weightsElemType, padValue, rewriter,
                                                                  weightsCst.getLoc()));
            }

            concatInputs.push_back(
                    rewriter.create<Const::DeclareOp>(weightsCst.getLoc(), groupWeights.getType(), groupWeights));

            if (groupIdx < groupNumb) {
                const auto padAfter = Shape{groupOutSize, (groupNumb - groupIdx) * groupInSize,
                                            weightsShape[Dims4D::Filter::KY], weightsShape[Dims4D::Filter::KX]};
                concatInputs.push_back(
                        createConstantOpForPadding(padAfter, weightsElemType, padValue, rewriter, weightsCst.getLoc()));
            }

            return rewriter.create<IE::ConcatOp>(weightsCst.getLoc(), concatInputs, Dims4D::Filter::IC).getResult();
        }

        const auto paddingBefore = Shape{0, (groupIdx - 1) * groupInSize, 0, 0};
        const auto paddingAfter = Shape{0, (groupNumb - groupIdx) * groupInSize, 0, 0};
        auto newGroupWeights = groupWeights.padWithZero(paddingBefore, paddingAfter);

        return rewriter.create<Const::DeclareOp>(weightsCst.getLoc(), newGroupWeights.getType(), newGroupWeights)
                .getResult();
    };

    SmallVector<mlir::Value> concatInputs;
    for (const auto& groupID : irange(groupNumb)) {
        concatInputs.push_back(reconstructGroupWeights(groupID + 1));
    }

    auto weightsConcat = rewriter.createOrFold<IE::ConcatOp>(weightsCst.getLoc(), concatInputs, Dims4D::Filter::OC);

    auto newWeights = weightsConcat;
    if (isWeightsHasFQ) {
        newWeights = rewriter.create<IE::FakeQuantizeOp>(weightsFQ.getLoc(), newWeights, weightsFQ.getInputLow(),
                                                         weightsFQ.getInputHigh(), weightsFQ.getOutputLow(),
                                                         weightsFQ.getOutputHigh(), weightsFQ.getLevels(),
                                                         weightsFQ.getAutoBroadcast())
                             .getResult();
        weightsFQ.replaceAllUsesWith(newWeights);
        weightsFQ.erase();
    }

    rewriter.replaceOpWithNewOp<IE::ConvolutionOp>(origOp, origOp.getInput(), newWeights, origOp.getBias(),
                                                   origOp.getStrides(), origOp.getPadsBegin(), origOp.getPadsEnd(),
                                                   origOp.getDilations(), nullptr, nullptr);

    return mlir::success();
}

//
// GroupConvToMultiConvConverter
//

class ConvertGroupConvToConvPass::GroupConvToMultiConvConverter final :
        public mlir::OpRewritePattern<IE::GroupConvolutionOp> {
public:
    GroupConvToMultiConvConverter(mlir::MLIRContext* ctx, mlir::PatternBenefit benefit, Logger log)
            : mlir::OpRewritePattern<IE::GroupConvolutionOp>(ctx, benefit), _log(log) {
    }

public:
    mlir::LogicalResult matchAndRewrite(IE::GroupConvolutionOp origOp, mlir::PatternRewriter& rewriter) const final;

private:
    Logger _log;
};

mlir::LogicalResult ConvertGroupConvToConvPass::GroupConvToMultiConvConverter::matchAndRewrite(
        IE::GroupConvolutionOp origOp, mlir::PatternRewriter& rewriter) const {
    _log.trace("Got GroupConvolutionOp layer at '{0}'", origOp->getLoc());
    VPUX_THROW_UNLESS(origOp.getType().getRank() == 4, "The pass currently can only support 4D input");

    const auto input = origOp.getInput();
    const auto inputShape = input.getType().cast<vpux::NDTypeInterface>().getShape();
    const auto weights = origOp.getFilter();
    const auto weightsShape = weights.getType().cast<vpux::NDTypeInterface>().getShape();
    const auto bias = origOp.getBias();
    const auto group = origOp.getGroups().value();
    const auto newInShape = Shape{inputShape[Dims4D::Act::N], inputShape[Dims4D::Act::C] / group,
                                  inputShape[Dims4D::Act::H], inputShape[Dims4D::Act::W]};
    const auto inputShapeAttr = getIntArrayAttr(getContext(), newInShape);
    const auto newWeightsShape = Shape{weightsShape[Dims4D::Filter::OC] / group, weightsShape[Dims4D::Filter::IC],
                                       weightsShape[Dims4D::Filter::KY], weightsShape[Dims4D::Filter::KX]};
    const auto weightsShapeAttr = getIntArrayAttr(getContext(), newWeightsShape);

    SmallVector<mlir::Value> slices;
    mlir::Value biasSlice;
    mlir::Value weightsSlice;
    for (const auto sliceIdx : irange(group)) {
        // Slice input
        Shape inputOffsets = Shape(inputShape.size(), 0);
        inputOffsets[Dims4D::Act::C] = checked_cast<int64_t>(inputShape[Dims4D::Act::C] / group * sliceIdx);
        const auto inputOffsetsAttr = getIntArrayAttr(getContext(), inputOffsets);
        const auto inputSlice =
                rewriter.createOrFold<IE::SliceOp>(origOp->getLoc(), input, inputOffsetsAttr, inputShapeAttr);

        // Slice weights
        Shape weightsOffsets = Shape(weightsShape.size(), 0);
        weightsOffsets[Dims4D::Filter::OC] = checked_cast<int64_t>(weightsShape[Dims4D::Filter::OC] / group * sliceIdx);
        const auto weightsOffsetsAttr = getIntArrayAttr(getContext(), weightsOffsets);
        auto fakeQuantizeOp = weights.getDefiningOp<IE::FakeQuantizeOp>();
        if (fakeQuantizeOp != nullptr) {
            const auto newFakeQuantizeParamShape = Shape{weightsShape[Dims4D::Filter::OC] / group, 1, 1, 1};
            const auto fakeQuantizeParamShapeAttr = getIntArrayAttr(getContext(), newFakeQuantizeParamShape);
            auto inputLow = fakeQuantizeOp.getInputLow();
            auto inputHigh = fakeQuantizeOp.getInputHigh();
            auto outputLow = fakeQuantizeOp.getOutputLow();
            auto outputHigh = fakeQuantizeOp.getOutputHigh();

            auto newInput = rewriter.createOrFold<IE::SliceOp>(fakeQuantizeOp->getLoc(), fakeQuantizeOp.getInput(),
                                                               weightsOffsetsAttr, weightsShapeAttr);
            if (inputLow.getType().cast<vpux::NDTypeInterface>().getShape()[Dims4D::Filter::OC] != 1) {
                inputLow = mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(rewriter.createOrFold<IE::SliceOp>(
                        fakeQuantizeOp->getLoc(), inputLow, weightsOffsetsAttr, fakeQuantizeParamShapeAttr));
            }
            if (outputLow.getType().cast<vpux::NDTypeInterface>().getShape()[Dims4D::Filter::OC] != 1) {
                outputLow = mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(rewriter.createOrFold<IE::SliceOp>(
                        fakeQuantizeOp->getLoc(), outputLow, weightsOffsetsAttr, fakeQuantizeParamShapeAttr));
            }
            if (inputHigh.getType().cast<vpux::NDTypeInterface>().getShape()[Dims4D::Filter::OC] != 1) {
                inputHigh = mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(rewriter.createOrFold<IE::SliceOp>(
                        fakeQuantizeOp->getLoc(), inputHigh, weightsOffsetsAttr, fakeQuantizeParamShapeAttr));
            }
            if (outputHigh.getType().cast<vpux::NDTypeInterface>().getShape()[Dims4D::Filter::OC] != 1) {
                outputHigh = mlir::cast<mlir::TypedValue<mlir::RankedTensorType>>(rewriter.createOrFold<IE::SliceOp>(
                        fakeQuantizeOp->getLoc(), outputHigh, weightsOffsetsAttr, fakeQuantizeParamShapeAttr));
            }

            weightsSlice = rewriter.create<IE::FakeQuantizeOp>(fakeQuantizeOp.getLoc(), newInput, inputLow, inputHigh,
                                                               outputLow, outputHigh, fakeQuantizeOp.getLevels(),
                                                               fakeQuantizeOp.getAutoBroadcast());
        } else {
            weightsSlice =
                    rewriter.createOrFold<IE::SliceOp>(origOp->getLoc(), weights, weightsOffsetsAttr, weightsShapeAttr);
        }

        // Slice Bias
        if (bias != nullptr) {
            auto biasShape = bias.getType().cast<vpux::NDTypeInterface>().getShape();
            const auto newBiasShape = Shape{biasShape[Dims4D::Act::N], biasShape[Dims4D::Act::C] / group,
                                            biasShape[Dims4D::Act::H], biasShape[Dims4D::Act::W]};
            const auto biasShapeAttr = getIntArrayAttr(getContext(), newBiasShape);
            Shape biasOffsets = Shape(biasShape.size(), 0);
            biasOffsets[Dims4D::Act::C] = checked_cast<int64_t>(newBiasShape[Dims4D::Act::C] * sliceIdx);
            const auto biasOffsetsAttr = getIntArrayAttr(getContext(), biasOffsets);
            biasSlice = rewriter.createOrFold<IE::SliceOp>(origOp->getLoc(), bias, biasOffsetsAttr, biasShapeAttr);
        } else {
            biasSlice = nullptr;
        }

        // New conv
        auto newConvLoc = appendLoc(origOp->getLoc(), "_ConvertGroupConv_{0}", sliceIdx);
        auto convOp = rewriter.create<IE::ConvolutionOp>(newConvLoc, inputSlice, weightsSlice, biasSlice,
                                                         origOp.getStrides(), origOp.getPadsBegin(),
                                                         origOp.getPadsEnd(), origOp.getDilations(), nullptr, nullptr);

        slices.push_back(convOp);
    }

    rewriter.replaceOpWithNewOp<IE::ConcatOp>(origOp, slices, Dims4D::Act::C.ind());

    return mlir::success();
}

//
// safeRunOnFunc
//

void ConvertGroupConvToConvPass::safeRunOnFunc() {
    auto& ctx = getContext();
    mlir::ConversionTarget target(ctx);

    target.addDynamicallyLegalOp<IE::GroupConvolutionOp>([&](IE::GroupConvolutionOp op) {
        return mlir::failed(IE::canConvertGroupConvToConv(op));
    });
    target.addLegalOp<IE::ConvolutionOp>();
    target.addLegalOp<IE::ReshapeOp>();
    target.addLegalOp<IE::ConcatOp>();
    target.addLegalOp<IE::SliceOp>();
    target.addLegalOp<Const::DeclareOp>();
    target.addLegalOp<IE::FakeQuantizeOp>();

    mlir::RewritePatternSet patterns(&ctx);
    patterns.add<GroupConvToSingleConvConverter>(&ctx, vpux::benefitHigh, _log);
    patterns.add<GroupConvToMultiConvConverter>(&ctx, vpux::benefitLow, _log);

    auto func = getOperation();
    if (mlir::failed(mlir::applyPartialConversion(func, target, std::move(patterns)))) {
        signalPassFailure();
    }
}

}  // namespace

//
// createConvertGroupConvToConvPass
//

std::unique_ptr<mlir::Pass> vpux::IE::createConvertGroupConvToConvPass(Logger log) {
    return std::make_unique<ConvertGroupConvToConvPass>(log);
}
