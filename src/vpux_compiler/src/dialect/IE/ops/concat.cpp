//
// Copyright (C) 2022 Intel Corporation.
// SPDX-License-Identifier: Apache 2.0
//

#include "vpux/compiler/dialect/IE/ops.hpp"

#include "vpux/compiler/dialect/IE/utils/elem_type_info_utils.hpp"
#include "vpux/compiler/utils/attributes.hpp"
#include "vpux/compiler/utils/error.hpp"
#include "vpux/compiler/utils/quantization.hpp"

#include "vpux/utils/IE/loop.hpp"
#include "vpux/utils/core/checked_cast.hpp"

#include <numeric>
#include <unordered_set>

using namespace vpux;

//
// build
//

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::ValueRange inputs,
                               ConcatAttr per_axis) {
    build(builder, state, inputs, per_axis, nullptr);
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::ValueRange inputs,
                               mlir::IntegerAttr axis, mlir::IntegerAttr offset, mlir::IntegerAttr stride) {
    build(builder, state, inputs, ConcatAttr::get(builder.getContext(), axis, offset, stride));
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::ValueRange inputs,
                               int64_t axis, int64_t offset, int64_t stride) {
    build(builder, state, inputs, getIntAttr(builder, axis), offset != 0 ? getIntAttr(builder, offset) : nullptr,
          stride != 1 ? getIntAttr(builder, stride) : nullptr);
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::ValueRange inputs, Dim axis,
                               int64_t offset, int64_t stride) {
    build(builder, state, inputs, axis.ind(), offset, stride);
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::Type outType,
                               mlir::ValueRange inputs, mlir::ArrayAttr static_offsets) {
    build(builder, state, outType, inputs, nullptr, static_offsets);
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::Type outType,
                               mlir::ValueRange inputs, ArrayRef<Shape> static_offsets) {
    const auto attrArr = to_small_vector(static_offsets | transformed([&](ShapeRef arr) -> mlir::Attribute {
                                             return getIntArrayAttr(builder, arr);
                                         }));

    build(builder, state, outType, inputs, builder.getArrayAttr(attrArr));
}

void vpux::IE::ConcatOp::build(mlir::OpBuilder& builder, mlir::OperationState& state, mlir::Type outType,
                               mlir::ValueRange inputs, ArrayRef<ShapeRef> static_offsets) {
    const auto attrArr = to_small_vector(static_offsets | transformed([&](ShapeRef arr) -> mlir::Attribute {
                                             return getIntArrayAttr(builder, arr);
                                         }));

    build(builder, state, outType, inputs, builder.getArrayAttr(attrArr));
}

//
// inferReturnTypeComponents
//

namespace {

mlir::FailureOr<Shape> inferOutShapeWithAxis(IE::ConcatOpAdaptor concat, mlir::Location loc) {
    const auto inType = concat.getInputs().front().getType().cast<vpux::NDTypeInterface>();
    const auto axis = normalizeAxis(concat);

    auto outShape = inType.getShape().toValues();

    for (const auto val : concat.getInputs().drop_front()) {
        const auto curShape = getShape(val);

        if (curShape.size() != outShape.size()) {
            return errorAt(loc, "Concat inputs have mismatched ranks: '{0}' vs '{1}'", curShape.size(),
                           outShape.size());
        }

        outShape[axis] += curShape[axis];
    }

    const auto perAxis = concat.getPerAxis().value();
    const auto offset = perAxis.getOffset() ? perAxis.getOffset().getValue().getSExtValue() : 0;
    const auto stride = perAxis.getStride() ? perAxis.getStride().getValue().getSExtValue() : 1;

    int64_t maxLatestIdx = -1;
    for (const auto idx : irange(concat.getInputs().size())) {
        const auto curShape = getShape(concat.getInputs()[idx]);
        const int64_t sizeByAxis = curShape[axis];
        const int64_t latestElemIdx = offset * idx + (sizeByAxis > 0 ? stride * (sizeByAxis - 1) : 0);
        maxLatestIdx = std::max(maxLatestIdx, latestElemIdx);
    }

    if (maxLatestIdx >= outShape[axis]) {
        return errorAt(loc, "Concat with offset '{0}' and stride '{1}' doesn't fit to output dimension '{2}'", offset,
                       stride, outShape[axis]);
    }

    return outShape;
}

mlir::FailureOr<Shape> inferReturnShapeWithOffsets(IE::ConcatOpAdaptor concat, mlir::Location loc) {
    if (!concat.getStaticOffsets().has_value()) {
        return errorAt(loc, "Missing static_offsets attribute");
    }

    const auto staticOffsets = concat.getStaticOffsets().value();
    if (staticOffsets.size() != concat.getInputs().size()) {
        return errorAt(loc, "Concat 'static_offsets' count '{0}' doesn't match inputs count '{1}'",
                       staticOffsets.size(), concat.getInputs().size());
    }

    const auto inType = concat.getInputs().front().getType().cast<vpux::NDTypeInterface>();
    const auto allOffsets = staticOffsets.getAsRange<mlir::ArrayAttr>();

    Shape outShape(checked_cast<size_t>(inType.getRank()), 0);

    for (const auto& p : zip(concat.getInputs(), allOffsets)) {
        const auto curVal = std::get<0>(p);
        const auto curShape = getShape(curVal);

        if (curShape.size() != outShape.size()) {
            return errorAt(loc, "Concat inputs have mismatched ranks: '{0}' vs '{1}'", curShape.size(),
                           outShape.size());
        }

        const auto curOffsets = Shape(parseIntArrayAttr<int64_t>(std::get<1>(p)));

        if (curOffsets.size() != curShape.size()) {
            return errorAt(loc, "Concat 'static_offsets' rank doesn't match its input");
        }

        for (const auto ind : irange(outShape.size())) {
            const auto d = Dim(ind);

            outShape[d] = std::max(outShape[d], curOffsets[d] + curShape[d]);
        }
    }

    // TODO: validate that inputs+static_offsets fully covers the output without intersections

    return outShape;
}

mlir::FailureOr<mlir::Type> inferReturnElemTypeWithAxis(IE::ConcatOpAdaptor concat, mlir::Location loc) {
    SmallVector<mlir::Type> types;
    const auto getElemTypeFromValue = [](mlir::Value operand) {
        return operand.getType().cast<vpux::NDTypeInterface>().getElementType();
    };
    std::transform(concat.getOperands().begin(), concat.getOperands().end(), std::back_inserter(types),
                   getElemTypeFromValue);

    const auto logCb = [loc](const formatv_object_base& msg) {
        std::ignore = errorAt(loc, "{0}", msg.str());
    };

    return inferOutElemTypeWithAxis(types, concat, logCb);
}

mlir::FailureOr<mlir::Type> inferReturnElemTypeWithOffsets(IE::ConcatOpAdaptor concat, ShapeRef outShape,
                                                           mlir::Location loc) {
    SmallVector<mlir::Type> types;
    const auto getElemTypeFromValue = [](mlir::Value operand) {
        return operand.getType().cast<vpux::NDTypeInterface>().getElementType();
    };
    std::transform(concat.getOperands().begin(), concat.getOperands().end(), std::back_inserter(types),
                   getElemTypeFromValue);

    const auto logCb = [loc](const formatv_object_base& msg) {
        std::ignore = errorAt(loc, "{0}", msg.str());
    };

    return inferOutElemTypeWithOffsets(types, concat, outShape, logCb);
}

}  // namespace

mlir::LogicalResult vpux::IE::ConcatOp::inferReturnTypeComponents(
        mlir::MLIRContext* ctx, std::optional<mlir::Location> optLoc, mlir::ValueShapeRange operands,
        mlir::DictionaryAttr attrs, mlir::OpaqueProperties, mlir::RegionRange,
        SmallVectorImpl<mlir::ShapedTypeComponents>& inferredReturnShapes) {
    const auto loc = optLoc.value_or(mlir::UnknownLoc::get(ctx));

    IE::ConcatOpAdaptor concat(operands, attrs);
    if (mlir::failed(concat.verify(loc))) {
        return mlir::failure();
    }

    if (concat.getInputs().empty()) {
        return errorAt(loc, "Missing inputs for '{0}'", IE::ConcatOp::getOperationName());
    }

    if (!concat.getPerAxis().has_value() && !concat.getStaticOffsets().has_value()) {
        return errorAt(loc, "Missing either 'per_axis' or 'static_offsets' attribute");
    }
    if (concat.getPerAxis().has_value() && concat.getStaticOffsets().has_value()) {
        return errorAt(loc, "Only one attribute ('per_axis' or 'static_offsets') should be provided");
    }

    const auto inType = concat.getInputs().front().getType().cast<mlir::RankedTensorType>();

    // Check consistent tensor attributes

    const auto inDesc = vpux::getTensorAttr(inType);

    for (const auto val : concat.getInputs().drop_front()) {
        const auto curType = val.getType().cast<mlir::RankedTensorType>();
        const auto curDesc = vpux::getTensorAttr(curType);

        if (curDesc != inDesc) {
            return errorAt(loc, "Misaligned TensorType attributes for '{0}' inputs", IE::ConcatOp::getOperationName());
        }
    }

    // Infer output shape

    const auto outShape =
            concat.getPerAxis() ? inferOutShapeWithAxis(concat, loc) : inferReturnShapeWithOffsets(concat, loc);
    if (mlir::failed(outShape)) {
        return mlir::failure();
    }

    // Infer output element type

    const auto outElemType = concat.getPerAxis() ? inferReturnElemTypeWithAxis(concat, loc)
                                                 : inferReturnElemTypeWithOffsets(concat, outShape.value(), loc);
    if (mlir::failed(outElemType)) {
        return mlir::failure();
    }

    // Return inferred components

    inferredReturnShapes.emplace_back(outShape.value().raw(), outElemType.value(), inDesc);
    return mlir::success();
}

//
// ConvertPerAxisToOffsets
//

namespace {

class ConvertPerAxisToOffsets final : public mlir::OpRewritePattern<IE::ConcatOp> {
public:
    using mlir::OpRewritePattern<IE::ConcatOp>::OpRewritePattern;

public:
    mlir::LogicalResult matchAndRewrite(IE::ConcatOp origOp, mlir::PatternRewriter& rewriter) const final;
};

const mlir::ArrayAttr inferOffsetsAttrWithAxis(IE::ConcatOp origOp, int64_t& axis) {
    auto rank = origOp.getOutput().getType().cast<vpux::NDTypeInterface>().getRank();

    SmallVector<SmallVector<int64_t>> finalOffsets;
    finalOffsets.push_back(SmallVector<int64_t>(rank, 0));
    int64_t correctAxis;
    if (axis < 0) {
        correctAxis = axis + rank;
    } else {
        correctAxis = axis;
    }
    for (auto input : origOp.getInputs() | indexed) {
        auto inputShape = getShape(input.value());
        auto offsets = SmallVector<int64_t>(rank, 0);
        offsets[correctAxis] = inputShape[Dim(correctAxis)] + finalOffsets.back()[correctAxis];
        finalOffsets.push_back(offsets);
    }
    finalOffsets.pop_back();

    return getIntArrayOfArray(origOp.getContext(), finalOffsets);
}

mlir::LogicalResult ConvertPerAxisToOffsets::matchAndRewrite(IE::ConcatOp origOp,
                                                             mlir::PatternRewriter& rewriter) const {
    if (origOp.getStaticOffsetsAttr()) {
        return mlir::failure();
    }

    if (origOp.getPerAxisAttr().getStride() || origOp.getPerAxisAttr().getOffset()) {
        return mlir::failure();
    }

    const auto outType = origOp.getOutput().getType().cast<vpux::NDTypeInterface>();
    auto axis = origOp.getPerAxisAttr().getAxis().getValue().getSExtValue();
    auto rank = origOp.getInputs().front().getType().cast<vpux::NDTypeInterface>().getRank();
    // Negative value means counting dimension from the end
    if (axis < 0) {
        axis += rank;
    }
    const auto finalOffsetsAttr = inferOffsetsAttrWithAxis(origOp, axis);

    rewriter.replaceOpWithNewOp<IE::ConcatOp>(origOp, outType, origOp.getInputs(), finalOffsetsAttr);
    return mlir::success();
}

}  // namespace

//
// FuseConcat
//

namespace {

class FuseConcat final : public mlir::OpRewritePattern<IE::ConcatOp> {
public:
    using OpRewritePattern::OpRewritePattern;

    mlir::LogicalResult matchAndRewrite(IE::ConcatOp op, mlir::PatternRewriter& rewriter) const final;
};

SmallVector<mlir::Value> getAllInputOp(IE::ConcatOp origOp, const std::unordered_set<Dim>& axis) {
    SmallVector<mlir::Value> inputOps;
    for (auto preOps : origOp.getInputs()) {
        auto producerConcatOp = preOps.getDefiningOp<IE::ConcatOp>();

        if (producerConcatOp != nullptr && producerConcatOp.getStaticOffsetsAttr()) {
            const auto subAxis = getConcatAxesFromOffsets(producerConcatOp, getShape(producerConcatOp.getOutput()));
            if (subAxis == axis) {
                for (auto inputTensor : producerConcatOp.getInputs()) {
                    inputOps.emplace_back(inputTensor);
                }
                continue;
            }
        }

        inputOps.emplace_back(preOps);
    }
    return inputOps;
}

mlir::LogicalResult FuseConcat::matchAndRewrite(IE::ConcatOp origOp, mlir::PatternRewriter& rewriter) const {
    if (origOp.getPerAxisAttr()) {
        return mlir::failure();
    }

    const auto axis = getConcatAxesFromOffsets(origOp, getShape(origOp.getOutput()));
    if (axis.size() != 1) {
        return mlir::failure();
    }

    const auto fuseInputs = getAllInputOp(origOp, axis);
    if (fuseInputs.size() <= origOp.getInputs().size()) {
        return mlir::failure();
    }

    const auto axisValue = *axis.begin();
    rewriter.replaceOpWithNewOp<IE::ConcatOp>(origOp, fuseInputs, axisValue.ind());

    return mlir::success();
}

}  // namespace

//
// FuseConstConcat
//

namespace {

class FuseConstConcat final : public mlir::OpRewritePattern<IE::ConcatOp> {
public:
    using OpRewritePattern::OpRewritePattern;

    mlir::LogicalResult matchAndRewrite(IE::ConcatOp op, mlir::PatternRewriter& rewriter) const final;
};

SmallVector<mlir::Value> getAllConstInputOp(IE::ConcatOp origOp) {
    SmallVector<mlir::Value> inputOps;
    for (auto preOps : origOp.getInputs()) {
        auto constOp = preOps.getDefiningOp<Const::DeclareOp>();

        if (constOp != nullptr) {
            inputOps.emplace_back(constOp);
        }
    }
    return inputOps;
}

mlir::LogicalResult FuseConstConcat::matchAndRewrite(IE::ConcatOp origOp, mlir::PatternRewriter& rewriter) const {
    // Convert below scenario to a Const
    //        Const  Const  Const
    //          |      |      |
    //           \     |     /         =>   Const
    //              Concat
    //                 |
    //
    if (origOp.getPerAxisAttr()) {
        return mlir::failure();
    }

    const auto constInputs = getAllConstInputOp(origOp);
    if (constInputs.size() != origOp.getInputs().size()) {
        return mlir::failure();
    }

    auto offsetAttr = parseIntArrayOfArrayAttr<uint64_t>(origOp.getStaticOffsets().value());
    if (offsetAttr.size() != constInputs.size()) {
        return mlir::failure();
    }

    const auto axis = getConcatAxesFromOffsets(origOp, getShape(origOp.getOutput()));
    if (axis.size() != 1) {
        return mlir::failure();
    }
    const auto axisValue = *axis.begin();

    auto outNdInterface = origOp.getOutput().getType().dyn_cast<vpux::NDTypeInterface>();
    auto output = Const::Content::allocTempBuffer(outNdInterface, outNdInterface.getElementType(), false);
    auto outBuf = output.getRawTempBuf();

    const auto elemSize = vpux::getElemTypeSize(outNdInterface.getElementType()).to<Byte>().count();

    auto outPhyShape = outNdInterface.getMemShape().raw();
    auto memDimIndex = outNdInterface.getDimsOrder().dimPos(axisValue);
    const auto preDims = std::accumulate(outPhyShape.begin(), outPhyShape.begin() + memDimIndex, (int64_t)1,
                                         std::multiplies<int64_t>());
    const auto afterDims = std::accumulate(outPhyShape.begin() + memDimIndex + 1, outPhyShape.end(), (int64_t)1,
                                           std::multiplies<int64_t>());
    const auto planeSizeInBytes = (afterDims * outPhyShape[memDimIndex]) * elemSize;

    loop_1d(LoopExecPolicy::Parallel, constInputs.size(), [&](int64_t inIndex) {
        auto cst = constInputs[inIndex].getDefiningOp<Const::DeclareOp>();
        auto content = cst.getContent();
        auto cstShape = content.getType().getShape();
        auto singleCopyElements = afterDims * cstShape[axisValue];
        auto singleCopyBytes = singleCopyElements * elemSize;
        auto planeOffset = offsetAttr[inIndex][axisValue.ind()] * afterDims * elemSize;
        const auto bufSize = checked_cast<size_t>(content.getType().getTotalAllocSize().count());
        std::vector<char> inBuf(bufSize);
        content.copyTo(MutableArrayRef(inBuf.data(), bufSize));
        loop_1d(LoopExecPolicy::Parallel, preDims, [&](uint64_t n) {
            std::copy_n(inBuf.data() + (n * singleCopyBytes), singleCopyBytes,
                        outBuf.data() + ((n * planeSizeInBytes) + planeOffset));
        });
    });

    const auto contentElemType = outNdInterface.getElementType();
    auto rankedTensorType = outNdInterface.cast<mlir::RankedTensorType>();
    mlir::DenseElementsAttr denseAttr;
    Const::ContentAttr contentAttr;
    if (auto qtype = contentElemType.dyn_cast<mlir::quant::QuantizedType>()) {
        rankedTensorType =
                outNdInterface.changeElemType(normalizeQuantStorageType(qtype)).cast<mlir::RankedTensorType>();
        denseAttr = mlir::DenseElementsAttr::getFromRawBuffer(rankedTensorType, output.getRawStorageBuf());
        contentAttr = Const::ContentAttr::get(denseAttr);
        contentAttr = contentAttr.quantCast(qtype);
    } else {
        denseAttr = mlir::DenseElementsAttr::getFromRawBuffer(rankedTensorType, output.getRawStorageBuf());
        contentAttr = Const::ContentAttr::get(denseAttr);
    }

    rewriter.replaceOpWithNewOp<Const::DeclareOp>(origOp, origOp.getType(), contentAttr);
    return mlir::success();
}

}  // namespace

//
// FuseSliceConcat
//

namespace {

class FuseSliceConcat final : public mlir::OpRewritePattern<IE::ConcatOp> {
public:
    using OpRewritePattern::OpRewritePattern;

    mlir::LogicalResult matchAndRewrite(IE::ConcatOp op, mlir::PatternRewriter& rewriter) const final;
};

bool doSlicesRepresentFullParent(ArrayRef<IE::SliceOp> sliceOps) {
    auto firstSlice = sliceOps[0];
    auto parentOp = firstSlice.getSource();
    const auto outShape = vpux::getShape(parentOp);
    auto processedShape = SmallVector<int64_t>(firstSlice.getStaticOffsets().size(), 0);
    auto compareCond = [](auto offset, auto procShape) {
        return (offset == 0 || offset == procShape);
    };
    auto processAxes = [](auto offset, auto dimShape) {
        return offset + dimShape;
    };
    auto isTrueCond = [](auto condition) {
        return condition;
    };
    for (auto sliceOp : sliceOps) {
        const auto offset = parseIntArrayAttr<int64_t>(sliceOp.getStaticOffsets());
        const auto shape = parseIntArrayAttr<int64_t>(sliceOp.getStaticSizes());
        SmallVector<bool> cond(offset.size(), false);
        std::transform(offset.begin(), offset.end(), processedShape.begin(), cond.begin(), compareCond);
        auto greaterThan1DimCount = std::count_if(offset.begin(), offset.end(), [](auto item) {
            return item > 1;
        });
        if (!std::all_of(cond.begin(), cond.end(), isTrueCond) || greaterThan1DimCount > 1) {
            return false;
        }
        std::transform(offset.begin(), offset.end(), shape.begin(), processedShape.begin(), processAxes);
    }

    SmallVector<int64_t> realInputShape = to_small_vector(outShape);
    SmallVector<bool> retCond(realInputShape.size(), false);
    std::transform(processedShape.begin(), processedShape.end(), realInputShape.begin(), retCond.begin(), compareCond);
    return std::all_of(retCond.begin(), retCond.end(), isTrueCond);
}

SmallVector<mlir::Value> getFoldInputsOp(IE::ConcatOp origOp) {
    SmallVector<IE::SliceOp> sameParentSliceOps;
    SmallVector<mlir::Value> inputOps;
    mlir::Value parent = nullptr;
    auto handleLastSlice = [&](mlir::Value sliceParent) {
        if (sameParentSliceOps.empty()) {
            return;
        }
        if (doSlicesRepresentFullParent(sameParentSliceOps)) {
            inputOps.emplace_back(sliceParent);
        } else {
            for (auto& sliceOp : sameParentSliceOps) {
                inputOps.emplace_back(sliceOp);
            }
        }
        sameParentSliceOps.clear();
    };
    for (const auto& perOps : origOp.getInputs()) {
        auto sliceOp = perOps.getDefiningOp<IE::SliceOp>();

        if (sliceOp != nullptr) {
            auto currentParent = sliceOp.getSource();
            if (currentParent != parent) {
                handleLastSlice(parent);
                parent = currentParent;
            }
            sameParentSliceOps.emplace_back(sliceOp);
        } else {
            handleLastSlice(parent);
            inputOps.emplace_back(perOps);
        }
    }
    // Process the concat's last parameter is SliceOp
    handleLastSlice(parent);
    return inputOps;
}

mlir::LogicalResult FuseSliceConcat::matchAndRewrite(IE::ConcatOp origOp, mlir::PatternRewriter& rewriter) const {
    //
    // Delete the Slice to avoid the Stride DMA when the sliceOps can represent the slice input.
    //             OP1
    //          /      \                        OP1     OP2
    //          |      |                         |       |
    //        Slice  Slice   OP2           =>    |       |
    //          |      |      |                  \       /
    //           \     |     /                     Concat
    //              Concat                           |
    //                 |
    //
    if (origOp.getPerAxisAttr()) {
        return mlir::failure();
    }

    const auto axis = getConcatAxesFromOffsets(origOp, getShape(origOp.getOutput()));
    if (axis.size() != 1) {
        return mlir::failure();
    }

    auto newInputs = getFoldInputsOp(origOp);
    if (newInputs.size() >= origOp.getInputs().size()) {
        return mlir::failure();
    }

    const auto axisValue = *axis.begin();
    if (newInputs.size() > 1) {
        rewriter.replaceOpWithNewOp<IE::ConcatOp>(origOp, newInputs, axisValue.ind());
    } else {
        rewriter.replaceAllUsesWith(origOp, newInputs[0]);
    }

    return mlir::success();
}

}  // namespace

//
// getCanonicalizationPatterns
//

void vpux::IE::ConcatOp::getCanonicalizationPatterns(mlir::RewritePatternSet& results, mlir::MLIRContext* ctx) {
    results.add<ConvertPerAxisToOffsets>(ctx);
    results.add<FuseConcat>(ctx);
    results.add<FuseSliceConcat>(ctx);
    results.add<FuseConstConcat>(ctx);
}

//
// fold
//

mlir::OpFoldResult vpux::IE::ConcatOp::fold(FoldAdaptor) {
    if (getInputs().size() == 1) {
        return getInputs().front();
    }

    return nullptr;
}
