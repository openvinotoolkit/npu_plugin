<!-- Autogenerated by mlir-tblgen; don't manually edit -->
### `-adjust-layouts`: Adjust required layouts for all layers
The pass is a part of `IECommon` pipeline.

This pass adds the required layouts instead of the default one
depending on the layer specification from underlying Dialect.
### `-convert-avg-pool-to-dw-conv`: Convert AvgPool op to GroupConvolution op
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces suitable `AvgPool` operations with `GroupConvolution` operation.
### `-convert-conv1d-to-conv2d`: Convert Convolution1D and GroupConvolution1D to its 2D variance
The pass is a part of `AdjustForVPU` pipeline.

Extends input, filter and output tensors with height = 1.
[N, C, W] -> [N, C, 1, W]
strides:    {2} -> strides:    {1, 2}
pads_begin: {2} -> pads_begin: {0, 2}
pads_end:   {2} -> pads_end:   {0, 2}
dilations:  {2} -> dilations:  {1, 2}
### `-convert-fc-to-conv`: Convert FullyConnected op to Convolution operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `FullyConnected` operations with `Convolution` operation.
It inserts extra `Reshape` operations to satisfy `Convolution` specification.
### `-convert-paddings-to-floor-mode`: Convert Convolution and Pooling layers paddings to FLOOR rouding mode
The pass is a part of `AdjustForVPU` pipeline.

This pass updates padding attributes for Convolution and Pooling layers.
It switches layer rounding mode to FLOOR and updates paddings to satisfy output shape.
### `-convert-precision-to-fp16`: Convert tensors precision from FP32 to FP16
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all FP32 tensors with FP16.
It updates both function bodies as well as Function signatures.
### `-convert-precision-to-i32`: Convert tensors precision from I64 to I32
The pass is a part of `AdjustForVPU` pipeline.
This pass replaces all I64 tensors with I32.
It updates both function bodies as well as Function signatures.
### `-convert-quantize-ops-to-eltwise`: Converts per-tensor Quantize/Dequantize to eltwise And mixed-precision operation
The pass is a part of `LowPrecision` pipeline.

Converts per-tensor Quantize/Dequantize to eltwise And mixed-precision operation
where input2 is input1 to perform type conversion on DPU instead of UPA.
### `-convert-scale-shift-depthwise`: Convert Scale-Shift operation to Depthwise Convolution
The pass is a part of `HardwareMode` pipeline.

Convert Scale-Shift operation to Depthwise convolution.
### `-convert-shape-to-4d`: Convert tensors shapes to 4D
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces ND tensor with 4D analogues for layers, which has such limitations on VPUIP level.
Also this pass replaces ND network inputs and outputs with 4D analogues to overcome runtime limitations.
### `-convert-tile-to-per-axis-tiles`: Convert tile op by multiple axes to multiple PerAxisTile operations
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `Tile` op with a set of `PerAxisTile` operations.
### `-convert-to-mem-permute`: Convert Reorder and Transpose ops to MemPermute operation
The pass is a part of `AdjustForVPU` pipeline.

This pass replaces all `Reorder` and `Transpose` operations with `MemPermute` operation.
### `-convert-weights-to-u8`: Shift data from a signed range to an unsigned one
The pass is a part of `LowPrecision` pipeline.

Pass detects quantized convolution and shifts weights data from a signed range to an unsigned one
### `-dequantize-const`: Dequantize constant tensors
The pass is a part of `LowPrecision` pipeline.

It performs constant folding for `Constant -> quant.dcast` case.
The pass is used as a fallback to FP16 computations for the cases, where quantized types where not used by layers.
### `-expand-activation-channels`: Allign input tensors shape of DPU operation with hardware requirements
The pass is a part of `buildHardwareModePipeline` pipeline.

This pass processes operations, which can be compile as a DPU tasks and
    expands channels number to number divisible by 16 in case they doesn't satisfy hardware requirements
### `-fuse-convert-with-quantize`: Fuse Convert with Quantize into QuantCast operation
Pass detects pattern Convert(i8/ui8 -> FP16) -> Quantize(FP16 -> !quant.uniform<...>)
and fuses it into single QuantCast(i8/ui8 -> !quant.uniform<...>) operation.
### `-fuse-post-ops`: Fuse activation functions with tasks that support post-processing
The pass is a part of `AdjustForVPU` pipeline.

Fuse activation functions (e.g. ReLU, leaky ReLU) with tasks that support post-processing
depending on the compilation mode
### `-fuse-quantized-ops`: Update quantize/dequantize ops
The pass is a part of `LowPrecision` pipeline.

Pass detects pattern quant.dcast -> op -> quant.qcast and converts it into single quantized Op
### `-handle-asymmetric-strides`: Handle operations with asymmetric strides
The pass is a part of `AdjustForVPU` pipeline.

This pass splits operations so that they are able to be infered with symmetric strides
    on dpu because of hardware limitation.
### `-handle-large-strides`: Handle operations with large strides
This pass splits operations with strides larger than supported on hardware.
### `-isolated-tiling`: Tile layers in isolation so that all their I/O fit into requirements
The pass performs simple tiling of the layers, which doesn't fit into memory requirements.

The pass tries to fit single layer in isolation, without any heuristics to run something
in parallel or continue computation in tiles.

The pass doesn't use any cost model and just choose the first largest tile size,
which fits into the memory requirements.
### `-merge-fake-quant`: Merge back to FakeQuantize
The pass is a part of `LowPrecision` pipeline.

It merges pair `quant.qcast -> quant.dcast` into single `IE.FakeQuantize`.
The pass is used as a fallback to FP16 computations for the cases, where quantized types where not used by layers.
### `-optimize-reorders`: Optimize extra Reorder operations
The pass is a part of `IECommon` pipeline.

This pass tries to optimize out Reorder operations for common cases
by propagating them from inputs to outputs and merging into layers.
### `-propagate-quantize-dequantize`: Propagate Quantize/Dequantize through agnostic operations
The pass is a part of LowPrecision pipeline.

Quantize/Dequantize are propagated through operations
### `-resolve-strided-slice`: Decouple strided slice to slice + reshape
The pass is a part of `AdjustForVPU` pipeline.
It replaces IE::StridedSlice with non zero masks to a simpler IE::StridedSlice with zero masks + IE::Reshape
It replaces IE::StridedSlice with dense<1> strides strides with a simple IE::Slice operation
### `-split-fake-quant`: Splits FakeQuantize
The pass is a part of `LowPrecision` pipeline.

It splits `FakeQuantize` operations to `quant.qcast -> quant.dcast` pair.
### `-unroll-batch`: Split FullyConnected inputs with multiple rows
This pass splits `FullyConnected` inputs by rows.

For example, `FullyConnected` input with 2x64 geometry will be split by two inputs with 1x64 dimensions.
Resulting vector rows go to corresponding `FullyConnected` operations and the outputs are concatenated.
### `-upstream-slice`: Optimization by upstreaming slice operations
Optimizes scenarios of IE::StridedSlice and IE::SliceOp without neighboring operations.
Moves the slice operations upwards through the graph, reducing both compute and memory usage.
In some cases the slice operation may be safely removed from the graph, if the action of upstreaming it
    only adapts the operations constants.
### `-use-user-layout`: Use user layouts for entry point function prototype
The pass is a part of `IECommon` pipeline.

This pass updates the CNNNetwork entry point function prototype
and uses user-provided layouts for its operands and results.
The pass inserts Reorder operations from/to topology layout.
### `-use-user-precision`: Use user precisions for entry point function prototype
The pass is a part of `IECommon` pipeline.

This pass updates the CNNNetwork entry point function prototype and use user-provided precisions for its operands and results.
The pass inserts Convert operations from/to topology precisions.
